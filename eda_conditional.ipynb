{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32387e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eda import get_objectives, get_constraints, non_dominated_sort, non_dominated, assign_crowding_distance, binary_tournament_selection, sample_population, cleanupsamples, generate_example_data, organize_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de26b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "import numpy as np\n",
    "from scipy.stats import gamma, norm\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from numba import jit\n",
    "import math\n",
    "from hdf5storage import loadmat, savemat\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3118f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_items_to_z(items):\n",
    "    alpha = np.empty(items.shape[1])\n",
    "    beta = np.empty(items.shape[1])\n",
    "    items_z = np.empty(items.shape)\n",
    "    for i in range(items.shape[1]):\n",
    "        a, loc, scale = gamma.fit(items[:, i], floc=0.0)\n",
    "        alpha[i] = a\n",
    "        beta[i] = scale\n",
    "        u = gamma.cdf(items[:,i], a = a, scale = scale)\n",
    "        u = np.clip(u, 1e-12, 1-1e-12)\n",
    "        items_z[:,i] = norm.ppf(u) \n",
    "    return alpha, beta, items_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52ee68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_long(YY, n_selected, n_obj, n_con):\n",
    "    rows = []\n",
    "    for j in range(n_selected):\n",
    "        for k in range(n_obj+n_con):\n",
    "            if k < n_obj:\n",
    "                rows.append(pd.DataFrame({\n",
    "                    'cumulative': YY[:,j, k],\n",
    "                    'step': f'Step {j}',\n",
    "                    'objective': f'Obj {k}'\n",
    "                }))\n",
    "            else:\n",
    "                rows.append(pd.DataFrame({\n",
    "                    'cumulative': YY[:,j, k],\n",
    "                    'step': f'Step {j}',\n",
    "                    'objective': f'Con {k-n_obj}',\n",
    "                }))\n",
    "    df_Y = pd.concat(rows, ignore_index=True)\n",
    "    return df_Y\n",
    "\n",
    "def ecdf(df_table,n_selected,n_obj,n_con):\n",
    "    nk = len(df_table)/(n_selected*(n_obj+n_con))\n",
    "    ecdf_table = np.zeros((n_selected,(n_obj+n_con),int(nk)))\n",
    "    for t in range(1,n_selected):\n",
    "        for i in range(n_obj+n_con):\n",
    "            if i < n_obj:\n",
    "                ecdf_table[t,i,:] = np.sort(df_table['cumulative'][(df_table['objective'] == f'Obj {i}')&(df_table['step'] == f'Step {t}')].values)\n",
    "            else:\n",
    "                ecdf_table[t,i,:] = np.sort(df_table['cumulative'][(df_table['objective'] == f'Con {i - n_obj}')&(df_table['step'] == f'Step {t}')].values)\n",
    "    return ecdf_table\n",
    "\n",
    "def z_transform_from_ecdf(Y, ecdf_table):\n",
    "    N, d = Y.shape  \n",
    "    Y_z = np.empty_like(Y)\n",
    "    for t in range(1,N):\n",
    "        for i in range(d):\n",
    "            ranks = np.searchsorted(ecdf_table[t,i,:], Y[t, i], side='right')\n",
    "            u = ranks / len(ecdf_table[t,i,:])\n",
    "            u = np.clip(u, 1e-12, 1 - 1e-12)\n",
    "            Y_z[t, i] = norm.ppf(u) \n",
    "    return Y_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85382a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_cumu_objectives(items, items_z, population, n_selected, n_obj, n_con, rng, if_inital): \n",
    "# can modify to use objectives instead of population\n",
    "    XX = np.empty((population.shape[0], n_selected, n_obj+n_con))\n",
    "    XX_z = np.empty((population.shape[0], n_selected, n_obj+n_con))\n",
    "    for k in range(population.shape[0]):\n",
    "        if if_inital:\n",
    "            qx = rng.permutation(population[k, :]) # permutation only for initial population\n",
    "        else:\n",
    "            qx = population[k, :]\n",
    "        XX[k,:,:] = items[qx,:]\n",
    "        XX_z[k,:,:] = items_z[qx,:]\n",
    "    YY = np.cumsum(XX, axis = 1)\n",
    "    # df_Y = convert_to_long(YY, n_selected, n_obj, n_con)\n",
    "    # ecdf_table = ecdf(df_Y, n_selected, n_obj, n_con)\n",
    "    # YY_z = np.empty_like(YY)\n",
    "    # for k in range(YY.shape[0]):\n",
    "    #     YY_z[k] = z_transform_from_ecdf(YY[k], ecdf_table)\n",
    "    #     YY_z[k,0,:] = XX_z[k,0,:]\n",
    "    YY_z = np.cumsum(XX_z, axis = 1)\n",
    "    return XX, XX_z, YY, YY_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f71c5a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_markov_in_y_by_t(X, Y):\n",
    "    K, N, d = X.shape\n",
    "    A_list = np.zeros((N-1, d, d))\n",
    "    b_list = np.zeros((N-1, d))\n",
    "    Q_list = np.zeros((N-1, d, d))\n",
    "    R2_list = np.zeros(N-1)\n",
    "    reg_list = []\n",
    "\n",
    "    for t in range(1, N):  \n",
    "        S_t = Y[:, t-1, :]  \n",
    "        Z_t = X[:, t,   :] \n",
    "        reg_t = LinearRegression(fit_intercept=True)\n",
    "        reg_t.fit(S_t, Z_t)\n",
    "        A_t = reg_t.coef_      # (d, d)\n",
    "        b_t = reg_t.intercept_ # (d,)\n",
    "        Z_hat_t = reg_t.predict(S_t)\n",
    "        R_t = Z_t - Z_hat_t\n",
    "        Q_t = np.cov(R_t, rowvar=False, bias=False)\n",
    "        r2 = reg_t.score(S_t, Z_t)\n",
    "\n",
    "        A_list[t-1, :, :] = A_t\n",
    "        b_list[t-1, :] = b_t\n",
    "        Q_list[t-1, :, :] = Q_t\n",
    "        R2_list[t-1] = r2\n",
    "        reg_list.append(reg_t)\n",
    "    params = {\"A\": A_list,\"b\": b_list,\"Q\": Q_list,\"regs\": reg_list,\"R2\": R2_list}\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa40195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_conditional(items, items_z, population, n_selected, n_obj, n_con, rng, if_inital):\n",
    "    objectives, objectives_z, cumu_objectives, cumu_objectives_z = get_norm_cumu_objectives(items, items_z, population, \n",
    "                                                                                                n_selected, n_obj, n_con, \n",
    "                                                                                                rng, if_inital)\n",
    "    dist_params = fit_markov_in_y_by_t(objectives_z, cumu_objectives_z)\n",
    "    return objectives_z, dist_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c571206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_density_given_Y_and_t(X_candidates, y_normal, params_time, t):\n",
    "    A_all = params_time[\"A\"]  \n",
    "    b_all = params_time[\"b\"]  \n",
    "    Q_all = params_time[\"Q\"]  \n",
    "\n",
    "    A_t = A_all[t-1]\n",
    "    b_t = b_all[t-1]\n",
    "    Q_t = Q_all[t-1]\n",
    "    X_candidates = np.asarray(X_candidates)\n",
    "    y_normal = np.asarray(y_normal).reshape(-1)\n",
    "    mean_t = A_t @ y_normal + b_t\n",
    "    Q_t = Q_t + 1e-1 * np.eye(Q_t.shape[0])\n",
    "\n",
    "    densities = mvn.pdf(X_candidates, mean=mean_t, cov=Q_t)\n",
    "    return densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c92f0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_rate_model(items_z, XX_0):\n",
    "    mean0 = np.mean(XX_0, axis = 0)\n",
    "    Sigma0 = np.cov(XX_0.T) \n",
    "    # add regularization to diagonal for singularity\n",
    "    Sigma0 = Sigma0 + 1e-1 * np.eye(Sigma0.shape[0])\n",
    "    mvn0 = mvn(mean=mean0, cov=Sigma0)\n",
    "    x_candidates = items_z\n",
    "    probabilities = mvn0.pdf(x_candidates)\n",
    "    probabilities = (probabilities+1e-12)/sum(probabilities+1e-12)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6a7b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_population_conditional(\n",
    "    samples, samples_z, objectives_z, dist_params,\n",
    "    pop_size, n_selected, capacity, rng): # no use of rng\n",
    "\n",
    "    pop_count = 0\n",
    "    population = np.zeros((pop_size, n_selected), dtype=np.int32)\n",
    "    n_items = samples.shape[0]\n",
    "\n",
    "    while pop_count < pop_size:\n",
    "        # select sequentially\n",
    "        knapsack = np.zeros(n_selected, dtype=int) # here knapsack is knapsack indices\n",
    "        for n in range(n_selected):\n",
    "            if n == 0: # select first item\n",
    "                probabilities = base_rate_model(samples_z, objectives_z[:, 0, :])\n",
    "                first_choice = rng.choice(n_items, p=probabilities)\n",
    "                first_item = samples_z[first_choice,:]\n",
    "                x_indices = np.setdiff1d(np.arange(n_items), knapsack)\n",
    "                y_prev = first_item \n",
    "                knapsack[0] = first_choice\n",
    "            else:\n",
    "                x_indices = np.setdiff1d(np.arange(n_items), knapsack[:n])\n",
    "                x_candidates = samples_z[x_indices, :]\n",
    "                densities = conditional_density_given_Y_and_t(\n",
    "                    x_candidates, y_prev, dist_params, n\n",
    "                )\n",
    "                probabilities = densities/sum(densities)\n",
    "                next_choice = rng.choice(len(probabilities), p=probabilities)\n",
    "                next_index = x_indices[next_choice]\n",
    "                next_item = samples_z[next_index,:]\n",
    "                knapsack[n] = next_index\n",
    "                y_prev = y_prev + next_item\n",
    "        \n",
    "        constraint = np.sum(samples[knapsack, -1])\n",
    "        if constraint <= capacity:\n",
    "            population[pop_count, :] = knapsack\n",
    "            pop_count += 1\n",
    "    \n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d878268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalize y during sampling\n",
    "# def sample_population_conditional(\n",
    "#     samples, samples_z, objectives_z, dist_params, ecdf_table, \n",
    "#     pop_size, n_selected, capacity, rng):\n",
    "\n",
    "#     pop_count = 0\n",
    "#     population = np.zeros((pop_size, n_selected), dtype=np.int32)\n",
    "#     n_items = samples.shape[0]\n",
    "\n",
    "#     while pop_count < pop_size:\n",
    "#         knapsack = np.zeros(n_selected, dtype=int)   \n",
    "#         y_prev_orig = None \n",
    "#         y_prev_z = None  \n",
    "#         for n in range(n_selected):\n",
    "#             if n == 0: \n",
    "#                 probabilities = base_rate_model(samples_z, objectives_z[:, 0, :])\n",
    "#                 first_choice = rng.choice(n_items, p=probabilities)\n",
    "#                 knapsack[0] = first_choice\n",
    "#                 y_prev_orig = samples[first_choice, :] \n",
    "#                 y_prev_z = samples_z[first_choice, :]\n",
    "#             else:\n",
    "#                 x_indices = np.setdiff1d(np.arange(n_items), knapsack[:n])\n",
    "#                 x_candidates = samples_z[x_indices, :]\n",
    "#                 if n == 1:\n",
    "#                     current_predictor_z = y_prev_z\n",
    "#                 else:\n",
    "#                     t = n - 1\n",
    "#                     current_predictor_z = np.empty_like(y_prev_orig)\n",
    "#                     for i in range(len(y_prev_orig)):\n",
    "#                         ranks = np.searchsorted(ecdf_table[t, i, :], y_prev_orig[i], side='right')\n",
    "#                         u = ranks / len(ecdf_table[t, i, :])\n",
    "#                         u = np.clip(u, 1e-12, 1 - 1e-12)\n",
    "#                         current_predictor_z[i] = norm.ppf(u)\n",
    "\n",
    "#                 densities = conditional_density_given_Y_and_t(\n",
    "#                     x_candidates, current_predictor_z, dist_params, n\n",
    "#                 )\n",
    "                \n",
    "#                 probabilities = densities/sum(densities)\n",
    "#                 next_choice = rng.choice(len(probabilities), p=probabilities)\n",
    "#                 next_index = x_indices[next_choice]\n",
    "#                 knapsack[n] = next_index\n",
    "#                 y_prev_orig = y_prev_orig + samples[next_index, :]\n",
    "        \n",
    "#         constraint = np.sum(samples[knapsack, -1])\n",
    "#         if constraint <= capacity:\n",
    "#             population[pop_count, :] = knapsack\n",
    "#             pop_count += 1\n",
    "    \n",
    "#     return population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5db27143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnapsackEDACond:\n",
    "    def __init__(self, items, capacity, n_selected, n_obj, n_con, pop_size=1000, \n",
    "                 generations=5, max_no_improve_gen=20, seed=1123):\n",
    "        self.items = items\n",
    "        self.items_z = None\n",
    "        self.capacity = capacity\n",
    "        self.n_selected = n_selected\n",
    "        self.n_obj = n_obj\n",
    "        self.n_con = n_con\n",
    "        self.pop_size = pop_size\n",
    "        self.generations = generations\n",
    "        self.max_no_improve_gen = max_no_improve_gen\n",
    "        self.rng = random.default_rng(seed=seed)\n",
    "        self.if_inital = True\n",
    "        self.ecdf_table = None\n",
    "        \n",
    "        # self.distribution = None\n",
    "        self.first_item_dist = None\n",
    "        self.distribution_params = None\n",
    "        self.selected_population = None  # (pop_size, n_selected)\n",
    "        self.selected_objectives = None  # (pop_size, n_obj) objective values are summed over solutions\n",
    "        self.objectives_z = None  # (pop_size, n_selected, n_obj)\n",
    "\n",
    "        self.distribution_params_table = []\n",
    "        self.pareto_indices_table = []\n",
    "        self.pareto_front_table = []\n",
    "        self.js_div_list = []\n",
    "        \n",
    "    def _generate_initial_population(self):\n",
    "        n_items = self.items.shape[0]\n",
    "        distribution = np.ones(n_items) / n_items\n",
    "        population = sample_population(\n",
    "            self.items, distribution, self.pop_size, self.n_selected, \n",
    "            self.capacity, self.rng\n",
    "        )\n",
    "        objectives = get_objectives(self.items, population, self.n_obj)\n",
    "        \n",
    "        ranks, fronts = non_dominated_sort(objectives)\n",
    "        distances_all_solutions = np.zeros(population.shape[0], dtype=float)\n",
    "        for f in fronts:\n",
    "            distances = assign_crowding_distance(objectives[f, :])\n",
    "            distances_all_solutions[f] = distances\n",
    "        \n",
    "        select_indices = np.array([], dtype=int)\n",
    "        while len(select_indices) < self.pop_size:\n",
    "            indice = binary_tournament_selection(\n",
    "                population, ranks, distances_all_solutions, self.rng\n",
    "            )\n",
    "            select_indices = np.concatenate([select_indices, np.array([indice])])\n",
    "        \n",
    "        selected_population = population[select_indices]\n",
    "        selected_objectives = objectives[select_indices]\n",
    "\n",
    "        _, _, self.items_z = transform_items_to_z(self.items)\n",
    "        self.objectives_z, self.distribution_params = fit_conditional(self.items, self.items_z, selected_population, \n",
    "                                                                        self.n_selected, self.n_obj, self.n_con,\n",
    "                                                                        self.rng, self.if_inital) # may need a different rng\n",
    "        self.first_item_dist = base_rate_model(self.items_z, self.objectives_z[:, 0, :])\n",
    "        self.selected_population = selected_population\n",
    "        self.selected_objectives = selected_objectives\n",
    "        \n",
    "        return #selected_population, selected_objectives, items_z, objectives_z, distribution_params\n",
    "    \n",
    "    def _update_distribution(self):\n",
    "        population = sample_population_conditional(\n",
    "            self.items, self.items_z, self.objectives_z, self.distribution_params, \n",
    "            self.pop_size, self.n_selected, self.capacity, self.rng\n",
    "        )\n",
    "        objectives = get_objectives(self.items, population, self.n_obj)\n",
    "        \n",
    "        # Find current pareto front\n",
    "        # _, _, _, fronts_current = non_dominated_sort(objectives)\n",
    "        _, fronts_current = non_dominated_sort(objectives)\n",
    "        pareto_indices = population[fronts_current[0]]\n",
    "        \n",
    "        objectives = np.vstack((self.selected_objectives, objectives))\n",
    "        population = np.vstack((self.selected_population, population))\n",
    "        \n",
    "        # _, _, ranks, fronts = non_dominated_sort(objectives)\n",
    "        ranks, fronts = non_dominated_sort(objectives)\n",
    "        select_indices = np.array([], dtype=np.int32)\n",
    "        for f in fronts:\n",
    "            if len(select_indices) + len(f) <= self.pop_size:\n",
    "                select_indices = np.concatenate([select_indices, f])\n",
    "            else:\n",
    "                remaining_size = self.pop_size - len(select_indices)\n",
    "                f_distance = assign_crowding_distance(objectives[f, :])\n",
    "                sort_indices = np.argsort(f_distance)[::-1]\n",
    "                remaining = f[sort_indices[:remaining_size]]\n",
    "                select_indices = np.concatenate([select_indices, remaining])\n",
    "                break\n",
    "        \n",
    "        selected_population = population[select_indices]\n",
    "        selected_objectives = objectives[select_indices]\n",
    "        n_training = int(self.pop_size*0.15)\n",
    "        training_population = selected_population[:n_training]\n",
    "        \n",
    "        # update distribution\n",
    "        self.objectives_z, self.distribution_params = fit_conditional(self.items, self.items_z, training_population, \n",
    "                                                                        self.n_selected, self.n_obj, self.n_con,\n",
    "                                                                        self.rng, self.if_inital)\n",
    "        \n",
    "        self.selected_population = selected_population\n",
    "        self.selected_objectives = selected_objectives\n",
    "\n",
    "        # check distribution convergence\n",
    "        updated_first_item_dist = base_rate_model(self.items_z, self.objectives_z[:, 0, :])\n",
    "        self.first_item_dist[self.first_item_dist < 1E-08] = 1E-08\n",
    "        updated_first_item_dist[updated_first_item_dist < 1E-08] = 1E-08\n",
    "        js_div = jensenshannon(self.first_item_dist, updated_first_item_dist)**2\n",
    "        self.first_item_dist = updated_first_item_dist\n",
    "                                                                        \n",
    "        return pareto_indices, js_div\n",
    "\n",
    "    def run(self):\n",
    "        self._generate_initial_population()\n",
    "        self.if_inital = False\n",
    "        \n",
    "        # Run generations (fixed number of generations)\n",
    "        for g in range(self.generations):\n",
    "            print(f\"Generation {g+1}/{self.generations}\")\n",
    "            pareto_indices, js_div = self._update_distribution()\n",
    "            print(f\"number of front 0: {pareto_indices.shape[0]}\")\n",
    "            \n",
    "            pareto_front = np.zeros((pareto_indices.shape[0], self.items.shape[1]))\n",
    "            for k in range(pareto_indices.shape[0]):\n",
    "                pareto_front[k, :] = np.sum(self.items[pareto_indices[k, :], :], axis=0)\n",
    "                \n",
    "            self.distribution_params_table.append(self.distribution_params.copy())\n",
    "            self.pareto_indices_table.append(pareto_indices.copy())\n",
    "            self.pareto_front_table.append(pareto_front.copy())\n",
    "            self.js_div_list.append(js_div)\n",
    "\n",
    "        return {\n",
    "            'distribution_params_table': self.distribution_params_table,\n",
    "            'pareto_indices_table': self.pareto_indices_table,\n",
    "            'pareto_front_table': self.pareto_front_table,\n",
    "            'js_div_list': self.js_div_list,\n",
    "            'objectives_z': self.objectives_z,\n",
    "            'items_z': self.items_z\n",
    "        }\n",
    "        \n",
    "        # # Run generations (until convergence)\n",
    "        # no_improve_gen = 0\n",
    "        # prev_js_div = None\n",
    "        # generation = 0\n",
    "        # while no_improve_gen < self.max_no_improve_gen:\n",
    "        #     generation += 1\n",
    "        #     print(f\"Generation {generation} (no improve count: {no_improve_gen})\")\n",
    "        #     pareto_indices, js_div = self._update_distribution()\n",
    "        #     print(f\"number of front 0: {pareto_indices.shape[0]}\")\n",
    "\n",
    "        #     pareto_front = np.zeros((pareto_indices.shape[0], self.items.shape[1]))\n",
    "        #     for k in range(pareto_indices.shape[0]):\n",
    "        #         pareto_front[k, :] = np.sum(self.items[pareto_indices[k, :], :], axis=0)\n",
    "                \n",
    "        #     self.distribution_params_table.append(self.distribution_params.copy())\n",
    "        #     self.pareto_indices_table.append(pareto_indices.copy())\n",
    "        #     self.pareto_front_table.append(pareto_front.copy())\n",
    "        #     self.js_div_list.append(js_div)\n",
    "                \n",
    "        #     if prev_js_div is not None:\n",
    "        #         diff = prev_js_div - js_div\n",
    "        #         if np.abs(diff) > 0.0001: # lowered criteria\n",
    "        #             no_improve_gen = 0\n",
    "        #         else:\n",
    "        #             no_improve_gen += 1\n",
    "        #     else:\n",
    "        #         no_improve_gen = 0\n",
    "        #     prev_js_div = js_div\n",
    "\n",
    "        # return {\n",
    "        #     'distribution_params_table': self.distribution_params_table,\n",
    "        #     'pareto_indices_table': self.pareto_indices_table,\n",
    "        #     'pareto_front_table': self.pareto_front_table,\n",
    "        #     'js_div_list': self.js_div_list\n",
    "        # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4416ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items_seed = 1211\n",
    "# # Generate data\n",
    "# items, rpos = generate_example_data(r, shape, scale, n_items=n_items, seed=items_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "244ce5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a test case\n",
    "kn = loadmat('/home/tailai/data/knapsack/runB/kn_1_1_allneg_60_6_3.mat')\n",
    "items = kn['items'][1]\n",
    "shape = kn['shape']\n",
    "scale = kn['scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a90d6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "eda_seed = 1223\n",
    "n_items = 60\n",
    "n_selected = 6\n",
    "n_obj = 3\n",
    "n_con = 1\n",
    "capacity = int(shape[-1]*scale[-1]*n_selected)\n",
    "pop_size = 1500\n",
    "generations = 50 # do not matter if check convergence\n",
    "max_no_improve_gen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2c1d146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1/50\n",
      "number of front 0: 32\n",
      "Generation 2/50\n",
      "number of front 0: 42\n",
      "Generation 3/50\n",
      "number of front 0: 61\n",
      "Generation 4/50\n",
      "number of front 0: 49\n",
      "Generation 5/50\n",
      "number of front 0: 63\n",
      "Generation 6/50\n",
      "number of front 0: 67\n",
      "Generation 7/50\n",
      "number of front 0: 74\n",
      "Generation 8/50\n",
      "number of front 0: 84\n",
      "Generation 9/50\n",
      "number of front 0: 69\n",
      "Generation 10/50\n",
      "number of front 0: 88\n",
      "Generation 11/50\n",
      "number of front 0: 86\n",
      "Generation 12/50\n",
      "number of front 0: 95\n",
      "Generation 13/50\n",
      "number of front 0: 87\n",
      "Generation 14/50\n",
      "number of front 0: 80\n",
      "Generation 15/50\n",
      "number of front 0: 80\n",
      "Generation 16/50\n",
      "number of front 0: 70\n",
      "Generation 17/50\n",
      "number of front 0: 81\n",
      "Generation 18/50\n",
      "number of front 0: 84\n",
      "Generation 19/50\n",
      "number of front 0: 74\n",
      "Generation 20/50\n",
      "number of front 0: 122\n",
      "Generation 21/50\n",
      "number of front 0: 144\n",
      "Generation 22/50\n",
      "number of front 0: 238\n",
      "Generation 23/50\n",
      "number of front 0: 270\n",
      "Generation 24/50\n",
      "number of front 0: 416\n",
      "Generation 25/50\n",
      "number of front 0: 519\n",
      "Generation 26/50\n",
      "number of front 0: 388\n",
      "Generation 27/50\n",
      "number of front 0: 546\n",
      "Generation 28/50\n",
      "number of front 0: 258\n",
      "Generation 29/50\n",
      "number of front 0: 288\n",
      "Generation 30/50\n",
      "number of front 0: 407\n",
      "Generation 31/50\n",
      "number of front 0: 320\n",
      "Generation 32/50\n",
      "number of front 0: 398\n",
      "Generation 33/50\n",
      "number of front 0: 290\n",
      "Generation 34/50\n",
      "number of front 0: 392\n",
      "Generation 35/50\n",
      "number of front 0: 257\n",
      "Generation 36/50\n",
      "number of front 0: 449\n",
      "Generation 37/50\n",
      "number of front 0: 391\n",
      "Generation 38/50\n",
      "number of front 0: 383\n",
      "Generation 39/50\n",
      "number of front 0: 396\n",
      "Generation 40/50\n",
      "number of front 0: 369\n",
      "Generation 41/50\n",
      "number of front 0: 317\n",
      "Generation 42/50\n",
      "number of front 0: 423\n",
      "Generation 43/50\n",
      "number of front 0: 405\n",
      "Generation 44/50\n",
      "number of front 0: 406\n",
      "Generation 45/50\n",
      "number of front 0: 387\n",
      "Generation 46/50\n",
      "number of front 0: 374\n",
      "Generation 47/50\n",
      "number of front 0: 491\n",
      "Generation 48/50\n",
      "number of front 0: 425\n",
      "Generation 49/50\n",
      "number of front 0: 387\n",
      "Generation 50/50\n",
      "number of front 0: 490\n"
     ]
    }
   ],
   "source": [
    "# Run EDA\n",
    "eda = KnapsackEDACond(\n",
    "    items=items,\n",
    "    capacity=capacity,\n",
    "    n_selected=n_selected,\n",
    "    n_obj=n_obj,\n",
    "    n_con=n_con,\n",
    "    pop_size=pop_size,\n",
    "    generations=generations,\n",
    "    max_no_improve_gen=max_no_improve_gen,\n",
    "    seed=eda_seed\n",
    ")\n",
    "\n",
    "#organize results    \n",
    "results = eda.run()\n",
    "with open('results_cond.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b579d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('results_cond.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7070a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_params = results['distribution_params_table'][-1]\n",
    "pareto_solutions = results['pareto_indices_table'][-1]\n",
    "objectives_z = results['objectives_z']\n",
    "items_z = results['items_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dadecd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converged_pf_from_dist(\n",
    "    dist_params, items, items_z, objectives_z, pareto_solutions, \n",
    "    capacity, n_selected, n_obj, f_seed=1234, \n",
    "    sample_size=1000, max_iters=100, max_no_change=2):\n",
    "\n",
    "    rng = np.random.default_rng(f_seed)       \n",
    " \n",
    "    # if len(pareto_solutions) > 0:\n",
    "    pareto_solutions = np.unique(np.sort(pareto_solutions, axis=1), axis=0)\n",
    "    pareto_objectives = get_objectives(items, pareto_solutions, n_obj)\n",
    "    # else:\n",
    "        # pareto_solutions = np.empty((0, n_selected), dtype=int)\n",
    "        # pareto_objectives = np.empty((0, n_obj))\n",
    "\n",
    "    no_change = 0\n",
    "    counter = 0\n",
    "    while no_change < max_no_change and counter < max_iters:\n",
    "        new_sample = sample_population_conditional(\n",
    "            items, items_z, objectives_z, dist_params,\n",
    "            sample_size, n_selected, capacity, rng\n",
    "        )\n",
    "        \n",
    "        # if len(pareto_solutions) == 0:\n",
    "        #      all_solutions = np.unique(np.sort(new_sample, axis=1), axis=0)\n",
    "        # else:\n",
    "        all_solutions = np.unique(np.sort(np.vstack((pareto_solutions, new_sample)), axis=1), axis=0)\n",
    "        all_objectives = get_objectives(items, all_solutions, n_obj)\n",
    "        nd_idx = non_dominated(all_objectives)\n",
    "        nd_idx = nd_idx.astype(bool)\n",
    "        new_pareto_solutions = all_solutions[nd_idx]\n",
    "        new_pareto_objectives = all_objectives[nd_idx]\n",
    "        \n",
    "        if np.array_equal(np.unique(new_pareto_objectives, axis=0), np.unique(pareto_objectives, axis=0)):\n",
    "            no_change += 1\n",
    "        else:\n",
    "            no_change = 0\n",
    "\n",
    "        pareto_solutions, pareto_objectives = new_pareto_solutions, new_pareto_objectives\n",
    "        counter += 1\n",
    "        print(f\"iter {counter}: {len(pareto_solutions)}\")\n",
    "    \n",
    "    return pareto_solutions, pareto_objectives, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97e27446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1: 112\n",
      "iter 2: 111\n",
      "iter 3: 113\n",
      "iter 4: 112\n",
      "iter 5: 119\n",
      "iter 6: 115\n",
      "iter 7: 121\n",
      "iter 8: 123\n",
      "iter 9: 124\n",
      "iter 10: 125\n",
      "iter 11: 125\n",
      "iter 12: 126\n",
      "iter 13: 127\n",
      "iter 14: 128\n",
      "iter 15: 129\n",
      "iter 16: 129\n",
      "iter 17: 131\n",
      "iter 18: 131\n",
      "iter 19: 132\n",
      "iter 20: 132\n",
      "iter 21: 132\n",
      "iter 22: 131\n",
      "iter 23: 131\n",
      "iter 24: 133\n",
      "iter 25: 134\n",
      "iter 26: 134\n",
      "iter 27: 134\n",
      "iter 28: 134\n",
      "iter 29: 134\n",
      "iter 30: 132\n",
      "iter 31: 132\n",
      "iter 32: 132\n",
      "iter 33: 132\n",
      "iter 34: 132\n",
      "iter 35: 132\n"
     ]
    }
   ],
   "source": [
    "pareto_solutions, pareto_objectives, counter = converged_pf_from_dist(\n",
    "    dist_params, items, items_z, objectives_z, pareto_solutions, \n",
    "    capacity, n_selected, n_obj, \n",
    "    max_no_change=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30a8d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdf5storage import loadmat, savemat\n",
    "kn= loadmat('/home/tailai/data/knapsack/runB/kn_1_1_allneg_60_6_3.mat')\n",
    "pareto_front_final = kn['pareto_front_final'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05dd565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of shared identical rows: 37\n"
     ]
    }
   ],
   "source": [
    "obj1 = pareto_objectives\n",
    "obj2 = pareto_front_final[:, :3]\n",
    "\n",
    "def view1D(a, b):\n",
    "    a = np.ascontiguousarray(a)\n",
    "    b = np.ascontiguousarray(b)\n",
    "    void_dt = np.dtype((np.void, a.dtype.itemsize * a.shape[1]))\n",
    "    return a.view(void_dt).ravel(),  b.view(void_dt).ravel()\n",
    "\n",
    "A, B = view1D(obj1, obj2)\n",
    "common_rows = np.intersect1d(A, B)\n",
    "count = len(common_rows)\n",
    "print(f\"Number of shared identical rows: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf889b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60267.64012499999\n",
      "73943.21762499999\n",
      "-0.1849470166331567\n"
     ]
    }
   ],
   "source": [
    "from pymoo.indicators.hv import HV\n",
    "A = obj1.astype(np.float64)\n",
    "B = obj2.astype(np.float64)\n",
    "A_min = -A\n",
    "B_min = -B\n",
    "\n",
    "worst_min = np.max(np.vstack([A_min, B_min]), axis=0)\n",
    "ref = worst_min * 1.05\n",
    "\n",
    "hv = HV(ref_point=ref)\n",
    "\n",
    "A_hv = hv(A_min)\n",
    "B_hv = hv(B_min)\n",
    "\n",
    "print(A_hv)\n",
    "print(B_hv)\n",
    "print((A_hv-B_hv)/B_hv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numba_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
