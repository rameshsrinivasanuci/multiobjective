{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef7934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eda import get_objectives, get_constraints, non_dominated_sort, non_dominated, assign_crowding_distance, binary_tournament_selection, sample_population, cleanupsamples, generate_example_data, organize_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6afa8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "import numpy as np\n",
    "from scipy.stats import gamma, norm\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from numba import jit\n",
    "import math\n",
    "from hdf5storage import loadmat, savemat\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a7baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_items_to_z(items):\n",
    "    alpha = np.empty(items.shape[1])\n",
    "    beta = np.empty(items.shape[1])\n",
    "    items_z = np.empty(items.shape)\n",
    "    items_r = items + 1\n",
    "    for i in range(items_r.shape[1]):\n",
    "        a, loc, scale = gamma.fit(items_r[:, i], floc=0.0)\n",
    "        alpha[i] = a\n",
    "        beta[i] = scale\n",
    "        u = gamma.cdf(items_r[:,i], a = a, scale = scale)\n",
    "        u = np.clip(u, 1e-12, 1-1e-12)\n",
    "        items_z[:,i] = norm.ppf(u) \n",
    "    return alpha, beta, items_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed4d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_gamma_y_to_z(YY, XX_z):\n",
    "    N, T, d = YY.shape\n",
    "    YY_z = np.empty((N, T, d), dtype=float)\n",
    "    YY_z[:, 0, :] = XX_z[:, 0, :] \n",
    "    shape = np.full((T-1, d), np.nan)\n",
    "    location = np.full((T-1, d), np.nan)\n",
    "    scale = np.full((T-1, d), np.nan)\n",
    "    for t in range(1, T):\n",
    "        for i in range(d):\n",
    "            y = YY[:, t, i]\n",
    "            y_r = y + 1\n",
    "\n",
    "            if np.ptp(y_r) < 1e-12:\n",
    "                raise RuntimeError(\n",
    "                    f\"Gamma degenerate at t={t}, i={i}\\n\"\n",
    "                    f\"unique(y_r)={np.unique(y_r)}\\n\"\n",
    "                    f\"min={y_r.min()}, max={y_r.max()}\\n\"\n",
    "                    f\"population size={YY.shape[0]}\"\n",
    "                    f\"XX_z= {XX_z[:,t,i]}\"\n",
    "                )\n",
    "\n",
    "            a, loc, b = gamma.fit(y_r, floc=0.0)\n",
    "            u = gamma.cdf(y_r, a=a, loc=loc, scale=b)\n",
    "            u = np.clip(u, 1e-12, 1 - 1e-12)\n",
    "            YY_z[:, t, i] = norm.ppf(u)\n",
    "            shape[t-1, i] = a\n",
    "            location[t-1, i] = loc\n",
    "            scale[t-1, i] = b\n",
    "    return YY_z, shape, location, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf9fddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_cumu_objectives(items, items_z, population, n_selected, n_obj, n_con, rng, if_initial):  # can modify to use objectives instead of population\n",
    "    XX = np.empty((population.shape[0], n_selected, n_obj+n_con))\n",
    "    XX_z = np.empty((population.shape[0], n_selected, n_obj+n_con))\n",
    "    for k in range(population.shape[0]):\n",
    "        if if_initial:\n",
    "            qx = rng.permutation(population[k, :]) # permutation only for initial population\n",
    "        else:\n",
    "            qx = population[k, :]\n",
    "        XX[k,:,:] = items[qx,:]\n",
    "        XX_z[k,:,:] = items_z[qx,:]\n",
    "    YY = np.cumsum(XX, axis = 1)\n",
    "    YY_z, shape, location, scale = fit_gamma_y_to_z(YY, XX_z)\n",
    "    return XX, XX_z, YY, YY_z, shape, location, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187744ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_markov_in_y_by_t(X, Y):\n",
    "    K, N, d = X.shape\n",
    "    A_list = np.zeros((N-1, d, d))\n",
    "    b_list = np.zeros((N-1, d))\n",
    "    Q_list = np.zeros((N-1, d, d))\n",
    "    R2_list = np.zeros(N-1)\n",
    "    reg_list = []\n",
    "\n",
    "    for t in range(1, N):  \n",
    "        S_t = Y[:, t-1, :]  \n",
    "        Z_t = X[:, t,   :] \n",
    "\n",
    "        reg_t = LinearRegression(fit_intercept=True)\n",
    "        reg_t.fit(S_t, Z_t)\n",
    "        A_t = reg_t.coef_      # (d, d)\n",
    "        b_t = reg_t.intercept_ # (d,)\n",
    "        Z_hat_t = reg_t.predict(S_t)\n",
    "        R_t = Z_t - Z_hat_t\n",
    "        Q_t = np.cov(R_t, rowvar=False, bias=False)\n",
    "        r2 = reg_t.score(S_t, Z_t)\n",
    "\n",
    "        A_list[t-1, :, :] = A_t\n",
    "        b_list[t-1, :] = b_t\n",
    "        Q_list[t-1, :, :] = Q_t\n",
    "        R2_list[t-1] = r2\n",
    "        reg_list.append(reg_t)\n",
    "    params = {\"A\": A_list,\"b\": b_list,\"Q\": Q_list,\"regs\": reg_list,\"R2\": R2_list}\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11878648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_conditional(items, items_z, population, n_selected, n_obj, n_con, rng, if_initial):\n",
    "    objectives, objectives_z, cumu_objectives, cumu_objectives_z, shape, location, scale = get_norm_cumu_objectives(items, items_z, population, \n",
    "                                                                                                                        n_selected, n_obj, n_con, \n",
    "                                                                                                                        rng, if_initial)\n",
    "    dist_params = fit_markov_in_y_by_t(objectives_z, cumu_objectives_z)\n",
    "    return objectives_z, dist_params, shape, location, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_density_given_Y_and_t(X_candidates, y_normal, params_time, t):\n",
    "    A_all = params_time[\"A\"]  \n",
    "    b_all = params_time[\"b\"]  \n",
    "    Q_all = params_time[\"Q\"]  \n",
    "\n",
    "    A_t = A_all[t-1]\n",
    "    b_t = b_all[t-1]\n",
    "    Q_t = Q_all[t-1]\n",
    "    X_candidates = np.asarray(X_candidates)\n",
    "    y_normal = np.asarray(y_normal).reshape(-1)\n",
    "    mean_t = A_t @ y_normal + b_t\n",
    "    Q_t = Q_t + 1e-3 * np.eye(Q_t.shape[0])\n",
    "    \n",
    "    densities = mvn.pdf(X_candidates, mean=mean_t, cov=Q_t)\n",
    "    return densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998ceee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_rate_model(items_z, XX_0):\n",
    "    mean0 = np.mean(XX_0, axis = 0)\n",
    "    Sigma0 = np.cov(XX_0.T) \n",
    "    # add regularization to diagonal for singularity\n",
    "    Sigma0 = Sigma0 + 1e-3 * np.eye(Sigma0.shape[0])\n",
    "    mvn0 = mvn(mean=mean0, cov=Sigma0)\n",
    "    x_candidates = items_z\n",
    "    probabilities = mvn0.pdf(x_candidates)\n",
    "    probabilities = (probabilities+1e-12)/sum(probabilities+1e-12)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a66eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_ratio_item(items, obj_to_max):\n",
    "    ratios = items[:, obj_to_max] / items[:, -1]\n",
    "    sort_indices = np.argsort(ratios)\n",
    "    best_item_index = sort_indices[-1]\n",
    "    return best_item_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4193e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_population_initial(samples, n_items, pop_size, n_selected, n_obj, n_con, capacity, rng):\n",
    "#     pop_count = 0\n",
    "#     population = np.zeros((pop_size, n_selected), dtype=np.int32)\n",
    "\n",
    "#     while pop_count < pop_size:\n",
    "#         knapsack_indices = np.full(n_selected, -1, dtype=int)  \n",
    "#         knapsack = np.zeros((n_selected, (n_obj + n_con)))\n",
    "#         for n in range(0, 4):\n",
    "#             x_indices = np.setdiff1d(np.arange(n_items), knapsack_indices[:n])\n",
    "#             x_candidates = samples[x_indices, :]\n",
    "#             obj_to_max = 0  \n",
    "\n",
    "#             next_choice_local = best_ratio_item(x_candidates, obj_to_max)\n",
    "#             next_index = x_indices[next_choice_local]\n",
    "#             knapsack_indices[n] = next_index\n",
    "#             knapsack[n, :] = samples[next_index, :]\n",
    "        \n",
    "#         for n in range(4, n_selected):\n",
    "#             x_indices = np.setdiff1d(np.arange(n_items), knapsack_indices[:n])\n",
    "#             x_candidates = samples[x_indices, :]\n",
    "#             probabilities = np.ones(x_candidates.shape[0])/ x_candidates.shape[0]\n",
    "            \n",
    "#             next_choice = rng.choice(len(probabilities), p=probabilities)\n",
    "#             next_index = x_indices[next_choice]\n",
    "#             knapsack_indices[n] = next_index\n",
    "#             knapsack[n, :] = samples[next_index, :]\n",
    "            \n",
    "#         constraint = np.sum(samples[knapsack_indices, -1])\n",
    "#         if constraint <= capacity:\n",
    "#             population[pop_count, :] = knapsack_indices\n",
    "#             pop_count += 1\n",
    "\n",
    "#     return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d3654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # parallel processing sampling population conditional\n",
    "# import multiprocessing\n",
    "# from functools import partial\n",
    "\n",
    "# def _sample_chunk(\n",
    "#     seed, chunk_size, \n",
    "#     samples, samples_z, dist_params, shape, location, scale, \n",
    "#     n_selected, n_obj, n_con, capacity, \n",
    "#     temperature=0.03, eps=1e-12, print_every=5000,\n",
    "# ):\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     n_items = samples.shape[0]\n",
    "#     population_chunk = np.zeros((chunk_size, n_selected), dtype=np.int32)\n",
    "\n",
    "#     pop_count = 0\n",
    "#     total_attempts = 0\n",
    "#     while pop_count < chunk_size:\n",
    "#         total_attempts += 1\n",
    "\n",
    "#         knapsack_indices = np.zeros(n_selected, dtype=int)\n",
    "#         knapsack = np.zeros((n_selected, (n_obj + n_con)))\n",
    "#         probabilities = np.ones(n_items, dtype=float) / n_items\n",
    "\n",
    "#         first_choice = rng.choice(n_items, p=probabilities)\n",
    "#         knapsack_indices[0] = first_choice\n",
    "#         knapsack[0, :] = samples[first_choice, :]\n",
    "#         y_prev_z = samples_z[first_choice, :].copy()\n",
    "#         y_cum = knapsack[0, :].copy()\n",
    "\n",
    "#         for n in range(1, n_selected):\n",
    "#             x_indices = np.setdiff1d(np.arange(n_items), knapsack_indices[:n])\n",
    "#             x_candidates_z = samples_z[x_indices, :]\n",
    "#             x_candidates = samples[x_indices, :]\n",
    "#             densities = conditional_density_given_Y_and_t(x_candidates_z, y_prev_z, dist_params, n)\n",
    "#             densities = np.asarray(densities, dtype=float)\n",
    "#             probabilities = densities / densities.sum()\n",
    "\n",
    "#             next_choice = rng.choice(len(probabilities), p=probabilities)\n",
    "#             next_index = x_indices[next_choice]\n",
    "#             knapsack_indices[n] = next_index\n",
    "#             knapsack[n, :] = samples[next_index, :]\n",
    "\n",
    "#             y_cum += knapsack[n, :]\n",
    "#             u = gamma.cdf(y_cum, a=shape[n - 1, :], loc=location[n - 1, :], scale=scale[n - 1, :])\n",
    "#             u = np.clip(u, eps, 1 - eps)\n",
    "#             y_prev_z = norm.ppf(u)\n",
    "\n",
    "#         constraint = np.sum(samples[knapsack_indices, -1])\n",
    "#         if constraint <= capacity:\n",
    "#             population_chunk[pop_count, :] = knapsack_indices\n",
    "#             pop_count += 1\n",
    "        \n",
    "#         if total_attempts % 5000 == 0:\n",
    "#             print(f\"  Worker pid={multiprocessing.current_process().pid}: \"\n",
    "#                   f\"Sampled {pop_count}/{chunk_size} (Attempts: {total_attempts})\", flush=True)\n",
    "                  \n",
    "#     return population_chunk\n",
    "\n",
    "\n",
    "# def sample_population_conditional_parallel(\n",
    "#     samples, samples_z, objectives_z, dist_params, \n",
    "#     shape, location, scale, if_converged, \n",
    "#     pop_size, n_selected, n_obj, n_con, capacity, rng, \n",
    "#     n_jobs=None, temperature=0.03, print_every=5000,\n",
    "# ):\n",
    "\n",
    "#     if n_jobs is None:\n",
    "#         n_jobs = multiprocessing.cpu_count()\n",
    "\n",
    "#     base = pop_size // n_jobs\n",
    "#     rem = pop_size % n_jobs\n",
    "#     chunks = [base] * n_jobs\n",
    "#     for i in range(rem):\n",
    "#         chunks[i] += 1\n",
    "#     chunks = [c for c in chunks if c > 0]\n",
    "#     n_jobs = len(chunks)\n",
    "\n",
    "#     seeds = rng.integers(0, 1_000_000_000, size=n_jobs, dtype=np.int64)\n",
    "#     with multiprocessing.Pool(processes=n_jobs) as pool:\n",
    "#         func = partial(\n",
    "#             _sample_chunk,\n",
    "#             samples=samples,\n",
    "#             samples_z=samples_z,\n",
    "#             dist_params=dist_params,\n",
    "#             shape=shape,\n",
    "#             location=location,\n",
    "#             scale=scale,\n",
    "#             n_selected=n_selected,\n",
    "#             n_obj=n_obj,\n",
    "#             n_con=n_con,\n",
    "#             capacity=capacity,\n",
    "#             temperature=temperature,\n",
    "#             print_every=print_every,\n",
    "#         )\n",
    "#         results = pool.starmap(func, zip(seeds.tolist(), chunks))\n",
    "\n",
    "#     population = np.vstack(results)\n",
    "#     return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# from functools import partial\n",
    "\n",
    "# def _sample_chunk_blend(\n",
    "#     seed, chunk_size, \n",
    "#     samples, samples_z, objectives_z, dist_params, \n",
    "#     shape, location, scale, if_converged,\n",
    "#     n_selected, n_obj, n_con, capacity\n",
    "# ):\n",
    "\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     n_items = samples.shape[0]\n",
    "#     population_chunk = np.zeros((chunk_size, n_selected), dtype=np.int32)\n",
    "#     pop_count = 0\n",
    "#     total_attempts = 0\n",
    "    \n",
    "#     while pop_count < chunk_size:\n",
    "#         total_attempts += 1\n",
    "        \n",
    "#         knapsack_indices = np.zeros(n_selected, dtype=int)    \n",
    "#         knapsack = np.zeros((n_selected, (n_obj + n_con)))\n",
    "#         y_cum = np.zeros((n_obj + n_con))\n",
    "#         for n in range(0, 1):\n",
    "#             x_indices = np.setdiff1d(np.arange(n_items), knapsack_indices[:n])\n",
    "#             x_candidates = samples[x_indices, :]\n",
    "#             obj_to_max = 0  \n",
    "\n",
    "#             next_choice_local = best_ratio_item(x_candidates, obj_to_max)\n",
    "#             next_index = x_indices[next_choice_local]\n",
    "#             knapsack_indices[n] = next_index\n",
    "#             knapsack[n, :] = samples[next_index, :]\n",
    "\n",
    "#             y_cum += knapsack[n, :]\n",
    "#             if n == 0:\n",
    "#                 y_prev_z = samples_z[next_index, :]\n",
    "#             else:\n",
    "#                 u = gamma.cdf(y_cum, a=shape[n-1, :], loc=location[n-1, :], scale=scale[n-1, :]) # bug at n = 0\n",
    "#                 u = np.clip(u, 1e-12, 1 - 1e-12)\n",
    "#                 y_prev_z = norm.ppf(u)\n",
    "\n",
    "#         for n in range(1, n_selected):\n",
    "#             x_indices = np.setdiff1d(np.arange(n_items), knapsack_indices[:n])   \n",
    "#             x_candidates_z = samples_z[x_indices, :]\n",
    "#             x_candidates = samples[x_indices, :]\n",
    "#             densities = conditional_density_given_Y_and_t(x_candidates_z, y_prev_z, dist_params, n)\n",
    "#             probabilities = densities / (densities.sum())\n",
    "            \n",
    "#             next_choice = rng.choice(len(probabilities), p=probabilities)\n",
    "#             next_index = x_indices[next_choice]\n",
    "#             knapsack_indices[n] = next_index\n",
    "#             knapsack[n, :] = samples[next_index, :]\n",
    "\n",
    "#             y_cum += knapsack[n, :]\n",
    "#             u = gamma.cdf(y_cum, a=shape[n-1, :], loc=location[n-1, :], scale=scale[n-1, :])\n",
    "#             u = np.clip(u, 1e-12, 1 - 1e-12)\n",
    "#             y_prev_z = norm.ppf(u)\n",
    "\n",
    "#         constraint = np.sum(samples[knapsack_indices, -1])\n",
    "#         if constraint <= capacity:\n",
    "#             population_chunk[pop_count, :] = knapsack_indices\n",
    "#             pop_count += 1\n",
    "        \n",
    "#         if total_attempts % 5000 == 0:\n",
    "#             print(f\"  Worker pid={multiprocessing.current_process().pid}: \"\n",
    "#                   f\"Sampled {pop_count}/{chunk_size} (Attempts: {total_attempts})\", flush=True)\n",
    "\n",
    "#     return population_chunk\n",
    "\n",
    "\n",
    "# def sample_population_conditional_parallel(\n",
    "#     samples, samples_z, objectives_z, dist_params, \n",
    "#     shape, location, scale, if_converged, \n",
    "#     pop_size, n_selected, n_obj, n_con, capacity, rng, \n",
    "#     n_jobs=None\n",
    "# ):\n",
    "\n",
    "#     if n_jobs is None:\n",
    "#         n_jobs = multiprocessing.cpu_count()\n",
    "        \n",
    "#     base = pop_size // n_jobs\n",
    "#     rem = pop_size % n_jobs\n",
    "#     chunks = [base] * n_jobs\n",
    "#     for i in range(rem):\n",
    "#         chunks[i] += 1\n",
    "#     chunks = [c for c in chunks if c > 0]\n",
    "#     n_jobs = len(chunks)\n",
    "\n",
    "#     seeds = rng.integers(0, 1_000_000_000, size=n_jobs, dtype=np.int64)\n",
    "\n",
    "#     with multiprocessing.Pool(processes=n_jobs) as pool:\n",
    "#         func = partial(\n",
    "#             _sample_chunk_blend,\n",
    "#             samples=samples,\n",
    "#             samples_z=samples_z,\n",
    "#             objectives_z=objectives_z,\n",
    "#             dist_params=dist_params,\n",
    "#             shape=shape,\n",
    "#             location=location,\n",
    "#             scale=scale,\n",
    "#             if_converged=if_converged,\n",
    "#             n_selected=n_selected,\n",
    "#             n_obj=n_obj,\n",
    "#             n_con=n_con,\n",
    "#             capacity=capacity,\n",
    "#         )\n",
    "#         results = pool.starmap(func, zip(seeds.tolist(), chunks))\n",
    "\n",
    "#     population = np.vstack(results)\n",
    "#     return population\n",
    "\n",
    "# def sample_population_conditional_converged_parallel(\n",
    "#     samples, samples_z, objectives_z, dist_params, \n",
    "#     shape, location, scale, if_converged, \n",
    "#     pop_size, n_selected, n_obj, n_con, capacity, rng, \n",
    "#     n_jobs=None\n",
    "# ):\n",
    "\n",
    "#     if n_jobs is None:\n",
    "#         n_jobs = multiprocessing.cpu_count()\n",
    "        \n",
    "#     base = pop_size // n_jobs\n",
    "#     rem = pop_size % n_jobs\n",
    "#     chunks = [base] * n_jobs\n",
    "#     for i in range(rem):\n",
    "#         chunks[i] += 1\n",
    "#     chunks = [c for c in chunks if c > 0]\n",
    "#     n_jobs = len(chunks)\n",
    "\n",
    "#     seeds = rng.integers(0, 1_000_000_000, size=n_jobs, dtype=np.int64)\n",
    "\n",
    "#     with multiprocessing.Pool(processes=n_jobs) as pool:\n",
    "#         func = partial(\n",
    "#             _sample_chunk_blend,\n",
    "#             samples=samples,\n",
    "#             samples_z=samples_z,\n",
    "#             objectives_z=objectives_z,\n",
    "#             dist_params=dist_params,\n",
    "#             shape=shape,\n",
    "#             location=location,\n",
    "#             scale=scale,\n",
    "#             if_converged=if_converged,\n",
    "#             n_selected=n_selected,\n",
    "#             n_obj=n_obj,\n",
    "#             n_con=n_con,\n",
    "#             capacity=capacity,\n",
    "#         )\n",
    "#         results = pool.starmap(func, zip(seeds.tolist(), chunks))\n",
    "\n",
    "#     population = np.vstack(results)\n",
    "#     return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ee932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_population_conditional(\n",
    "    samples, samples_z, objectives_z, dist_params,\n",
    "    shape, location, scale, if_converged,\n",
    "    pop_size, n_selected, n_obj, n_con, capacity, rng):\n",
    "\n",
    "    pop_count = 0\n",
    "    population = np.zeros((pop_size, n_selected), dtype=np.int32)\n",
    "    n_items = samples.shape[0]\n",
    "    weights_all = samples[:, -1]\n",
    "\n",
    "    first_sample_choices_list = None\n",
    "    first_sample_probabilities_list = None\n",
    "    while pop_count < pop_size:\n",
    "        knapsack_indices = np.zeros(n_selected, dtype=int)    \n",
    "        knapsack = np.zeros((n_selected,(n_obj+n_con)))\n",
    "        choices_list = [] # record the choice and probabilities of a sample\n",
    "        probabilities_list = []\n",
    "\n",
    "        used_weight = 0.0              \n",
    "        failed = False\n",
    "        feas0 = np.flatnonzero(weights_all <= capacity) \n",
    "        if feas0.size == 0:\n",
    "            raise ValueError(\"No item fits within capacity.\")             \n",
    "\n",
    "        probabilities = np.ones(n_items) / n_items # sample first item from uniform\n",
    "        first_choice = rng.choice(n_items, p=probabilities)\n",
    "        knapsack_indices[0] = first_choice\n",
    "        knapsack[0, :] = samples[first_choice, :]\n",
    "        y_prev_z = samples_z[first_choice, :] \n",
    "        y_cum = knapsack[0, :].copy()\n",
    "        used_weight += float(weights_all[first_choice])   \n",
    "        choices_list.append(first_choice)\n",
    "        probabilities_list.append(probabilities)\n",
    "   \n",
    "        for n in range(1, n_selected):\n",
    "            remain = capacity - used_weight              \n",
    "            if remain <= 0:\n",
    "                failed = True\n",
    "                break\n",
    "\n",
    "            x_indices = np.setdiff1d(np.arange(n_items), knapsack_indices[:n])   \n",
    "            feas_mask = (weights_all[x_indices] <= remain)    \n",
    "            x_indices = x_indices[feas_mask]            \n",
    "            if x_indices.size == 0:                     \n",
    "                failed = True\n",
    "                break\n",
    "            \n",
    "            x_candidates = samples_z[x_indices, :] \n",
    "            densities = conditional_density_given_Y_and_t(x_candidates, y_prev_z, dist_params, n)\n",
    "            # added for robustness\n",
    "            densities = np.asarray(densities, dtype=float).ravel()\n",
    "            den_sum = densities.sum()\n",
    "            probabilities = densities / den_sum if den_sum > 0 else np.ones_like(densities) / len(densities)\n",
    "            # probabilities = densities/sum(densities)  \n",
    "\n",
    "            next_choice = rng.choice(len(probabilities), p=probabilities)\n",
    "            next_index = x_indices[next_choice]\n",
    "            knapsack_indices[n] = next_index\n",
    "            knapsack[n, :] = samples[next_index, :]\n",
    "            used_weight += float(weights_all[next_index]) \n",
    "            choices_list.append(next_index)\n",
    "            probabilities_list.append(probabilities)\n",
    "            \n",
    "            # normalize y\n",
    "            y_cum += knapsack[n, :]\n",
    "            u = gamma.cdf(y_cum, a=shape[n-1, :], loc=location[n-1, :], scale=scale[n-1, :])\n",
    "            u = np.clip(u, 1e-12, 1-1e-12)\n",
    "            y_prev_z = norm.ppf(u)\n",
    "        \n",
    "        if failed:                                  \n",
    "            continue\n",
    "        \n",
    "        constraint = np.sum(samples[knapsack_indices, -1])\n",
    "        if constraint <= capacity:\n",
    "            population[pop_count, :] = knapsack_indices\n",
    "            pop_count += 1\n",
    "\n",
    "            if pop_count == 1:\n",
    "                first_sample_choices_list = choices_list\n",
    "                first_sample_probabilities_list = probabilities_list\n",
    "    \n",
    "    return population, first_sample_choices_list, first_sample_probabilities_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1072b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_population_conditional_converged(\n",
    "    samples, samples_z, objectives_z, dist_params,\n",
    "    shape, location, scale, if_converged,\n",
    "    pop_size, n_selected, n_obj, n_con, capacity, rng):\n",
    "\n",
    "    pop_count = 0\n",
    "    population = np.zeros((pop_size, n_selected), dtype=np.int32)\n",
    "    n_items = samples.shape[0]\n",
    "    weights_all = samples[:, -1]\n",
    "\n",
    "    first_sample_choices_list = None\n",
    "    first_sample_probabilities_list = None\n",
    "    while pop_count < pop_size:\n",
    "        knapsack_indices = np.zeros(n_selected, dtype=int)    \n",
    "        knapsack = np.zeros((n_selected,(n_obj+n_con)))\n",
    "        choices_list = [] # record the choice and probabilities of a sample\n",
    "        probabilities_list = []\n",
    "\n",
    "        used_weight = 0.0              \n",
    "        failed = False           \n",
    "\n",
    "        y_cum = np.zeros((n_obj + n_con))\n",
    "        for n in range(0, 1):\n",
    "            remain = capacity - used_weight\n",
    "            if remain <= 0:\n",
    "                failed = True\n",
    "                break\n",
    "\n",
    "            x_indices = np.setdiff1d(np.arange(n_items), knapsack_indices[:n])  \n",
    "            feas_mask = (weights_all[x_indices] <= remain)    \n",
    "            x_indices = x_indices[feas_mask]            \n",
    "            if x_indices.size == 0:                     \n",
    "                failed = True\n",
    "                break\n",
    "\n",
    "            x_candidates = samples[x_indices, :]\n",
    "            obj_to_max = 0  \n",
    "            next_choice_local = best_ratio_item(x_candidates, obj_to_max)\n",
    "            next_index = x_indices[next_choice_local]\n",
    "            knapsack_indices[n] = next_index\n",
    "            knapsack[n, :] = samples[next_index, :]\n",
    "\n",
    "            used_weight += float(weights_all[next_index])\n",
    "            y_cum += knapsack[n, :]\n",
    "            if n == 0:\n",
    "                y_prev_z = samples_z[next_index, :]\n",
    "            else:\n",
    "                u = gamma.cdf(y_cum, a=shape[n-1, :], loc=location[n-1, :], scale=scale[n-1, :]) # bug at n = 0\n",
    "                u = np.clip(u, 1e-12, 1 - 1e-12)\n",
    "                y_prev_z = norm.ppf(u)\n",
    "\n",
    "            choices_list.append(next_index)\n",
    "\n",
    "        if failed:\n",
    "            continue\n",
    "\n",
    "        for n in range(1, n_selected):\n",
    "            remain = capacity - used_weight              \n",
    "            if remain <= 0:\n",
    "                failed = True\n",
    "                break\n",
    "\n",
    "            x_indices = np.setdiff1d(np.arange(n_items), knapsack_indices[:n])  \n",
    "            feas_mask = (weights_all[x_indices] <= remain)    \n",
    "            x_indices = x_indices[feas_mask]            \n",
    "            if x_indices.size == 0:                     \n",
    "                failed = True\n",
    "                break\n",
    "\n",
    "            x_candidates_z = samples_z[x_indices, :] \n",
    "            x_candidates = samples[x_indices, :] \n",
    "            densities = conditional_density_given_Y_and_t(x_candidates_z, y_prev_z, dist_params, n)\n",
    "            # added for robustness\n",
    "            densities = np.asarray(densities, dtype=float).ravel()\n",
    "            den_sum = densities.sum()\n",
    "            probabilities = densities / den_sum if den_sum > 0 else np.ones_like(densities) / len(densities)\n",
    "            # probabilities = densities/sum(densities) \n",
    "            \n",
    "            next_choice = rng.choice(len(probabilities), p=probabilities)\n",
    "            next_index = x_indices[next_choice]\n",
    "            knapsack_indices[n] = next_index\n",
    "            knapsack[n, :] = samples[next_index, :]\n",
    "            \n",
    "            used_weight += float(weights_all[next_index]) \n",
    "            choices_list.append(next_index)\n",
    "            probabilities_list.append(probabilities)\n",
    "            \n",
    "            # normalize y\n",
    "            y_cum += knapsack[n, :]\n",
    "            u = gamma.cdf(y_cum, a=shape[n-1, :], loc=location[n-1, :], scale=scale[n-1, :])\n",
    "            u = np.clip(u, 1e-12, 1-1e-12)\n",
    "            y_prev_z = norm.ppf(u)\n",
    "        \n",
    "        constraint = np.sum(samples[knapsack_indices, -1])\n",
    "        if constraint <= capacity:\n",
    "            population[pop_count, :] = knapsack_indices\n",
    "            pop_count += 1\n",
    "\n",
    "            if pop_count == 1:\n",
    "                first_sample_choices_list = choices_list\n",
    "                first_sample_probabilities_list = probabilities_list\n",
    "    \n",
    "    return population, first_sample_choices_list, first_sample_probabilities_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd525cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnapsackEDACond:\n",
    "    def __init__(self, items, capacity, n_selected, n_obj, n_con, pop_size=1000, \n",
    "                 generations=10, max_no_improve_gen=10, max_iters=100, seed=1123):\n",
    "        self.items = items\n",
    "        self.capacity = capacity\n",
    "        self.n_selected = n_selected\n",
    "        self.n_obj = n_obj\n",
    "        self.n_con = n_con\n",
    "        self.pop_size = pop_size\n",
    "        self.generations = generations\n",
    "        self.max_no_improve_gen = max_no_improve_gen\n",
    "        self.max_iters = max_iters\n",
    "        self.rng = random.default_rng(seed=seed)\n",
    "\n",
    "        self.items_z = None\n",
    "        self.shape = None\n",
    "        self.location = None\n",
    "        self.scale = None\n",
    "        self.if_initial = True\n",
    "        self.if_converged = False\n",
    "        self.first_item_dist = None\n",
    "        self.distribution_params = None\n",
    "        self.selected_population = None  # (pop_size, n_selected)\n",
    "        self.selected_objectives = None  # (pop_size, n_obj) objective values are summed over solutions\n",
    "        self.objectives_z = None  # (pop_size, n_selected, n_obj)\n",
    "        self.first_sample_choices_list = None\n",
    "        self.first_sample_probabilities_list = None\n",
    "\n",
    "        self.distribution_params_table = []\n",
    "        self.pareto_indices_table = []\n",
    "        self.pareto_front_table = []\n",
    "        self.js_div_list = []\n",
    "        self.converged_pf_table = []\n",
    "        self.selected_objectives_table = []\n",
    "        self.pareto_front_accu_table = []\n",
    "        self.pareto_indices_accu_table = []\n",
    "        self.first_sample_choices_list_table = []\n",
    "        self.first_sample_probabilities_list_table = []\n",
    "\n",
    "\n",
    "    def _generate_initial_population(self):\n",
    "        n_items = self.items.shape[0]\n",
    "        distribution = np.ones(n_items) / n_items\n",
    "\n",
    "        population = sample_population(\n",
    "            self.items, distribution, self.pop_size, self.n_selected, #self.n_obj, self.n_con,\n",
    "            self.capacity, self.rng\n",
    "        )\n",
    "        objectives = get_objectives(self.items, population, self.n_obj)\n",
    "        \n",
    "        ranks, fronts = non_dominated_sort(objectives)\n",
    "        distances_all_solutions = np.zeros(population.shape[0], dtype=float)\n",
    "        for f in fronts:\n",
    "            distances = assign_crowding_distance(objectives[f, :])\n",
    "            distances_all_solutions[f] = distances\n",
    "\n",
    "        select_indices = np.array([], dtype=int)\n",
    "        while len(select_indices) < self.pop_size:\n",
    "            indice = binary_tournament_selection(\n",
    "                population, ranks, distances_all_solutions, self.rng\n",
    "            )\n",
    "            select_indices = np.concatenate([select_indices, np.array([indice])])\n",
    "        \n",
    "        selected_population = population[select_indices]\n",
    "        selected_objectives = objectives[select_indices]\n",
    "\n",
    "        _, _, self.items_z = transform_items_to_z(self.items)\n",
    "        self.objectives_z, self.distribution_params, self.shape, self.location, self.scale = fit_conditional(self.items, self.items_z, selected_population, \n",
    "                                                                                                                self.n_selected, self.n_obj, self.n_con,\n",
    "                                                                                                                self.rng, self.if_initial) # may need a different rng\n",
    "        self.first_item_dist = base_rate_model(self.items_z, self.objectives_z[:, 0, :])\n",
    "        self.selected_population = selected_population\n",
    "        self.selected_objectives = selected_objectives\n",
    "\n",
    "    \n",
    "    def _update_distribution(self):\n",
    "        # sampling\n",
    "        population,first_sample_choices_list, first_sample_probabilities_list = sample_population_conditional(\n",
    "            self.items, self.items_z, self.objectives_z, self.distribution_params,\n",
    "            self.shape, self.location, self.scale, self.if_converged,\n",
    "            self.pop_size, self.n_selected, self.n_obj, self.n_con, self.capacity, self.rng\n",
    "        )\n",
    "        objectives = get_objectives(self.items, population, self.n_obj)\n",
    "\n",
    "        # store first sample choices and probabilities\n",
    "        self.first_sample_choices_list = first_sample_choices_list\n",
    "        self.first_sample_probabilities_list = first_sample_probabilities_list\n",
    "        \n",
    "        # find current pareto front\n",
    "        _, fronts_current = non_dominated_sort(objectives)\n",
    "        pareto_indices = population[fronts_current[0]]\n",
    "        \n",
    "        # stack populations\n",
    "        objectives = np.vstack((self.selected_objectives, objectives))\n",
    "        population = np.vstack((self.selected_population, population))\n",
    "        \n",
    "        # select through non-dominated sorting\n",
    "        ranks, fronts = non_dominated_sort(objectives)\n",
    "        select_indices = np.array([], dtype=np.int32)\n",
    "        for f in fronts:\n",
    "            if len(select_indices) + len(f) <= self.pop_size:\n",
    "                select_indices = np.concatenate([select_indices, f])\n",
    "            else:\n",
    "                remaining_size = self.pop_size - len(select_indices)\n",
    "                f_distance = assign_crowding_distance(objectives[f, :])\n",
    "                sort_indices = np.argsort(f_distance)[::-1]\n",
    "                remaining = f[sort_indices[:remaining_size]]\n",
    "                select_indices = np.concatenate([select_indices, remaining])\n",
    "                break\n",
    "        selected_population = population[select_indices]\n",
    "        selected_objectives = objectives[select_indices]\n",
    "        \n",
    "        n_training = int(self.pop_size*0.15)\n",
    "        training_population = selected_population[:n_training]\n",
    "        \n",
    "        # update distribution\n",
    "        self.objectives_z, self.distribution_params, self.shape, self.location, self.scale= fit_conditional(self.items, self.items_z, training_population, \n",
    "                                                                                                                self.n_selected, self.n_obj, self.n_con,\n",
    "                                                                                                                self.rng, self.if_initial)\n",
    "        \n",
    "        self.selected_population = selected_population\n",
    "        self.selected_objectives = selected_objectives\n",
    "\n",
    "        # compute js divergence\n",
    "        updated_first_item_dist = base_rate_model(self.items_z, self.objectives_z[:, 0, :])\n",
    "        self.first_item_dist[self.first_item_dist < 1E-08] = 1E-08\n",
    "        updated_first_item_dist[updated_first_item_dist < 1E-08] = 1E-08\n",
    "        js_div = jensenshannon(self.first_item_dist, updated_first_item_dist)**2\n",
    "        self.first_item_dist = updated_first_item_dist\n",
    "        \n",
    "        return pareto_indices, js_div\n",
    "\n",
    "    def _converged_pf(self):\n",
    "        # sampling\n",
    "        population, first_sample_choices_list, first_sample_probabilities_list = sample_population_conditional_converged(\n",
    "            self.items, self.items_z, self.objectives_z, self.distribution_params,\n",
    "            self.shape, self.location, self.scale, self.if_converged,\n",
    "            self.pop_size, self.n_selected, self.n_obj, self.n_con, self.capacity, self.rng)\n",
    "        objectives = get_objectives(self.items, population, self.n_obj)\n",
    "\n",
    "        # store first sample choices and probabilities\n",
    "        self.first_sample_choices_list = first_sample_choices_list\n",
    "        self.first_sample_probabilities_list = first_sample_probabilities_list\n",
    "\n",
    "        # find current pareto front\n",
    "        pareto_indices = population[non_dominated(objectives).astype(bool)]\n",
    "        \n",
    "        # stack populations\n",
    "        population = np.unique(np.sort(np.vstack((self.selected_population, population)), axis=1), axis=0)\n",
    "        objectives = get_objectives(self.items, population, self.n_obj)\n",
    "\n",
    "        # select through non-dominated\n",
    "        nd_idx = non_dominated(objectives).astype(bool)\n",
    "        selected_population = population[nd_idx]\n",
    "        selected_objectives = objectives[nd_idx]\n",
    "\n",
    "        # update distribution\n",
    "        self.objectives_z, self.distribution_params, self.shape, self.location, self.scale = fit_conditional(self.items, self.items_z, selected_population, \n",
    "                                                                                                                self.n_selected, self.n_obj, self.n_con,\n",
    "                                                                                                                self.rng, self.if_initial)\n",
    "        self.selected_population = selected_population\n",
    "        self.selected_objectives = selected_objectives    \n",
    "\n",
    "        # compute js divergence\n",
    "        updated_first_item_dist = base_rate_model(self.items_z, self.objectives_z[:, 0, :])\n",
    "        self.first_item_dist[self.first_item_dist < 1E-08] = 1E-08\n",
    "        updated_first_item_dist[updated_first_item_dist < 1E-08] = 1E-08\n",
    "        js_div = jensenshannon(self.first_item_dist, updated_first_item_dist)**2\n",
    "        self.first_item_dist = updated_first_item_dist                                                                         \n",
    "        \n",
    "        return pareto_indices, js_div\n",
    "\n",
    "    def run(self):\n",
    "        self._generate_initial_population()\n",
    "        self.if_initial = False\n",
    "        \n",
    "        # # Run generations (until convergence)\n",
    "        # part 1: train on a portion of selected population till base rate converges\n",
    "        no_improve_gen = 0\n",
    "        prev_js_div = None\n",
    "        generation = 0\n",
    "        min_gens = 30\n",
    "        while no_improve_gen < self.max_no_improve_gen:\n",
    "        # for generation in range(self.generations):\n",
    "            generation += 1\n",
    "            print(f\"Generation {generation} (no improve count: {no_improve_gen})\")\n",
    "            pareto_indices, js_div = self._update_distribution()\n",
    "            print(f\"number of front 0: {pareto_indices.shape[0]}\")\n",
    "\n",
    "            pareto_front = np.zeros((pareto_indices.shape[0], self.items.shape[1]))\n",
    "            for k in range(pareto_indices.shape[0]):\n",
    "                pareto_front[k, :] = np.sum(self.items[pareto_indices[k, :], :], axis=0)\n",
    "                \n",
    "            self.distribution_params_table.append(self.distribution_params.copy())\n",
    "            self.pareto_indices_table.append(pareto_indices.copy())\n",
    "            self.pareto_front_table.append(pareto_front.copy())\n",
    "            self.js_div_list.append(js_div)\n",
    "            self.selected_objectives_table.append(self.selected_objectives.copy())\n",
    "            self.first_sample_choices_list_table.append(self.first_sample_choices_list)\n",
    "            self.first_sample_probabilities_list_table.append(self.first_sample_probabilities_list)\n",
    "            \n",
    "                \n",
    "            # convergence criteria\n",
    "            if prev_js_div is not None:\n",
    "                diff = prev_js_div - js_div\n",
    "                if generation > min_gens and np.abs(diff) < 0.005: \n",
    "                # if np.abs(diff) < 0.005:\n",
    "                    no_improve_gen += 1\n",
    "                else:\n",
    "                    no_improve_gen = 0\n",
    "            else:\n",
    "                no_improve_gen = 0\n",
    "            prev_js_div = js_div\n",
    "        \n",
    "        # part 2: train on only the non-dominated solutions till pareto front converges\n",
    "        no_improve_gen = 0\n",
    "        counter = 0\n",
    "        prev_front_0 = None\n",
    "        pareto_indices_accu = None\n",
    "        while no_improve_gen < self.max_no_improve_gen and counter < self.max_iters:\n",
    "            counter += 1\n",
    "            print(f\"Iterations {counter} (no improve count: {no_improve_gen})\")\n",
    "            pareto_indices, js_div = self._converged_pf()\n",
    "            print(f\"number of front 0: {pareto_indices.shape[0]}\")\n",
    "\n",
    "            pareto_front = np.zeros((pareto_indices.shape[0], self.items.shape[1]))\n",
    "            for k in range(pareto_indices.shape[0]):\n",
    "                pareto_front[k, :] = np.sum(self.items[pareto_indices[k, :], :], axis=0)\n",
    "\n",
    "            self.distribution_params_table.append(self.distribution_params.copy())\n",
    "            self.pareto_indices_table.append(pareto_indices.copy())\n",
    "            self.pareto_front_table.append(pareto_front.copy())\n",
    "            self.js_div_list.append(js_div)\n",
    "            self.selected_objectives_table.append(self.selected_objectives.copy())\n",
    "            self.first_sample_choices_list_table.append(self.first_sample_choices_list)\n",
    "            self.first_sample_probabilities_list_table.append(self.first_sample_probabilities_list)\n",
    "            \n",
    "            front_0 = np.unique(self.selected_objectives, axis=0)\n",
    "            front_0 = front_0[np.lexsort(front_0.T[::-1])]\n",
    "            # convergence criteria\n",
    "            if prev_front_0 is not None:\n",
    "                if np.array_equal(prev_front_0, front_0):\n",
    "                    no_improve_gen += 1\n",
    "                else:\n",
    "                    no_improve_gen = 0\n",
    "            else:\n",
    "                no_improve_gen = 0\n",
    "            \n",
    "            self.converged_pf_table.append(front_0.copy())\n",
    "            prev_front_0 = front_0\n",
    "\n",
    "            # accumulate pareto front\n",
    "            if pareto_indices_accu is None:\n",
    "                pareto_indices_accu = pareto_indices.copy()\n",
    "                pareto_front_accu = get_objectives(self.items, pareto_indices_accu, self.n_obj)\n",
    "            else:\n",
    "                pareto_indices_combined = np.unique(np.sort(np.vstack((pareto_indices_accu, pareto_indices)), axis=1), axis=0)\n",
    "                pareto_front_combined = get_objectives(self.items, pareto_indices_combined, self.n_obj)\n",
    "                pf_ind = non_dominated(pareto_front_combined).astype(bool)\n",
    "                pareto_indices_accu = pareto_indices_combined[pf_ind]\n",
    "                pareto_front_accu = pareto_front_combined[pf_ind]\n",
    "            self.pareto_front_accu_table.append(pareto_front_accu.copy())\n",
    "            self.pareto_indices_accu_table.append(pareto_indices_accu.copy())\n",
    "\n",
    "\n",
    "        return {\n",
    "            'distribution_params_table': self.distribution_params_table,\n",
    "            'pareto_indices_table': self.pareto_indices_table,\n",
    "            'pareto_front_table': self.pareto_front_table,\n",
    "            'converged_pf_table': self.converged_pf_table,\n",
    "            'js_div_list': self.js_div_list,\n",
    "            'objectives_z': self.objectives_z,\n",
    "            'items_z': self.items_z,\n",
    "            'shape': self.shape,\n",
    "            'location': self.location,\n",
    "            'scale': self.scale,\n",
    "            'base_rate_dist': self.first_item_dist,\n",
    "            'selected_objectives_table': self.selected_objectives_table,\n",
    "            'first_sample_choices_list_table': self.first_sample_choices_list_table,\n",
    "            'first_sample_probabilities_list_table': self.first_sample_probabilities_list_table,\n",
    "            'pareto_front_accu_table': self.pareto_front_accu_table,\n",
    "            'pareto_indices_accu_table': self.pareto_indices_accu_table,\n",
    "            'mode 1 generations': generation,\n",
    "            'mode 2 generations': counter\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e930f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdf5storage import loadmat, savemat\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7251eba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a test case\n",
    "kn = loadmat('/data/knapsack/runA/kn_2_3_allneg_120_12_4.mat')\n",
    "run = 0\n",
    "items = kn['items'][run]\n",
    "shape = kn['shape']\n",
    "scale = kn['scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8441516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "eda_seed = 1223\n",
    "n_items = 120\n",
    "n_selected = 12\n",
    "n_obj = 4\n",
    "n_con = 1\n",
    "capacity = int(shape[-1]*scale[-1]*n_selected)\n",
    "pop_size = 1000\n",
    "generations = 100 # do not matter if check convergence\n",
    "max_no_improve_gen = 5\n",
    "max_iters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c2498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run EDA\n",
    "eda = KnapsackEDACond(\n",
    "    items=items,\n",
    "    capacity=capacity,\n",
    "    n_selected=n_selected,\n",
    "    n_obj=n_obj,\n",
    "    n_con=n_con,\n",
    "    pop_size=pop_size,\n",
    "    generations=generations,\n",
    "    max_no_improve_gen=max_no_improve_gen,\n",
    "    max_iters=max_iters,\n",
    "    seed=eda_seed\n",
    ")\n",
    "\n",
    "# organize results    \n",
    "results = eda.run()\n",
    "with open(f'blend_2_3_120_12_obj4_run{run}_test.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9da43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'results_final/final_2_3_{n_items}_{n_selected}_obj{n_obj}_run{run}.pkl', 'rb') as f:\n",
    "    results_full = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57946f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'blend_2_3_{n_items}_{n_selected}_obj{n_obj}_run{run}.pkl', 'rb') as f:\n",
    "#     results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de2914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_pf = results['pareto_front_accu_table'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628b446",
   "metadata": {},
   "outputs": [],
   "source": [
    "converged_pf_full = results_full['converged_pf_table'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690cbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results['pareto_front_accu_table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b4adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results['pareto_front_table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib widget\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot(converged_pf[:,0], converged_pf[:,1], converged_pf[:,2], 'rs', alpha=0.5, markersize=5)\n",
    "ax.plot(converged_pf_full[:,0], converged_pf_full[:,1], converged_pf_full[:,2], 'go', alpha=0.7, markersize=5)\n",
    "ax.set_xlabel('Obj 1')\n",
    "ax.set_ylabel('Obj 2')\n",
    "ax.set_zlabel('Obj 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2d6152",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(converged_pf[:,0], converged_pf[:,1], 'rs', alpha=0.7, markersize=5)\n",
    "ax.plot(converged_pf_full[:,0], converged_pf_full[:,1], 'go', alpha=0.2, markersize=5)\n",
    "ax.set_xlabel('Obj 1')\n",
    "ax.set_ylabel('Obj 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61ba223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.indicators.hv import HV\n",
    "A = converged_pf[:,0:n_obj].astype(np.float64)\n",
    "B = converged_pf_full[:,0:n_obj].astype(np.float64)\n",
    "A_min = -A\n",
    "B_min = -B\n",
    "\n",
    "# worst_min = np.max(np.vstack([A_min, B_min]), axis=0)\n",
    "# ref = worst_min * 1.05\n",
    "ref = np.array([0.0, 0.0, 0.0])\n",
    "hv = HV(ref_point=ref)\n",
    "\n",
    "A_hv = hv(A_min)\n",
    "B_hv = hv(B_min)\n",
    "\n",
    "print(A_hv)\n",
    "print(B_hv)\n",
    "print((A_hv-B_hv)/B_hv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0325d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_predicted = converged_pf[:, 0:n_obj]\n",
    "pf_actual = converged_pf_full[:, 0:n_obj]\n",
    "dominated = np.zeros(len(pf_predicted))\n",
    "for i in range(len(pf_predicted)):\n",
    "    for j in range(len(pf_actual)):\n",
    "        if np.all(pf_actual[j, :n_obj] >= pf_predicted[i, :n_obj]) and \\\n",
    "            np.any(pf_actual[j, :n_obj] > pf_predicted[i, :n_obj]):\n",
    "            dominated[i] = 1\n",
    "            break\n",
    "np.sum(dominated)/len(pf_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf74911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_plots = len(results['pareto_front_table']) \n",
    "cols = 5\n",
    "rows = math.ceil(n_plots / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*4))\n",
    "axes = np.atleast_1d(axes).ravel()\n",
    "\n",
    "for k, i in enumerate(range(len(results['pareto_front_table']))):\n",
    "    ax = axes[k]\n",
    "\n",
    "    ax.plot(converged_pf[:,0].copy(),      converged_pf[:,1].copy(),      'rs', alpha=0.7, markersize=5)\n",
    "    # ax.plot(converged_pf_full[:,0].copy(), converged_pf_full[:,1].copy(), 'go', alpha=0.2, markersize=5)\n",
    "\n",
    "    pf = results['pareto_front_table'][i]\n",
    "    ax.plot(pf[:,0].copy(), pf[:,1].copy(), marker='o', linestyle='None',\n",
    "        markerfacecolor='none', markeredgecolor='k',\n",
    "        markersize=5)\n",
    "    ax.set_title(f\"i={i}\")\n",
    "    ax.set_xlabel(\"Obj 1\")\n",
    "    ax.set_ylabel(\"Obj 2\")\n",
    "\n",
    "for ax in axes[n_plots:]:\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f425e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numba_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
