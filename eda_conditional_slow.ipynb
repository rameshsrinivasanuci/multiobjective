{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32387e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eda import get_objectives, get_constraints, non_dominated_sort, non_dominated, assign_crowding_distance, binary_tournament_selection, sample_population, cleanupsamples, generate_example_data, organize_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de26b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "import numpy as np\n",
    "from scipy.stats import gamma, norm\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from numba import jit\n",
    "import math\n",
    "from hdf5storage import loadmat, savemat\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3118f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_items_to_z(items):\n",
    "    alpha = np.empty(items.shape[1])\n",
    "    beta = np.empty(items.shape[1])\n",
    "    items_z = np.empty(items.shape)\n",
    "    items_r = items + 1\n",
    "    for i in range(items_r.shape[1]):\n",
    "        a, loc, scale = gamma.fit(items_r[:, i], floc=0.0)\n",
    "        alpha[i] = a\n",
    "        beta[i] = scale\n",
    "        u = gamma.cdf(items_r[:,i], a = a, scale = scale)\n",
    "        u = np.clip(u, 1e-12, 1-1e-12)\n",
    "        items_z[:,i] = norm.ppf(u) \n",
    "    return alpha, beta, items_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52ee68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_long(YY, n_selected, n_obj, n_con):\n",
    "    rows = []\n",
    "    for j in range(n_selected):\n",
    "        for k in range(n_obj+n_con):\n",
    "            if k < n_obj:\n",
    "                rows.append(pd.DataFrame({\n",
    "                    'cumulative': YY[:,j, k],\n",
    "                    'step': f'Step {j}',\n",
    "                    'objective': f'Obj {k}'\n",
    "                }))\n",
    "            else:\n",
    "                rows.append(pd.DataFrame({\n",
    "                    'cumulative': YY[:,j, k],\n",
    "                    'step': f'Step {j}',\n",
    "                    'objective': f'Con {k-n_obj}',\n",
    "                }))\n",
    "    df_Y = pd.concat(rows, ignore_index=True)\n",
    "    return df_Y\n",
    "\n",
    "def ecdf(df_table,n_selected,n_obj,n_con):\n",
    "    nk = len(df_table)/(n_selected*(n_obj+n_con))\n",
    "    ecdf_table = np.zeros((n_selected,(n_obj+n_con),int(nk)))\n",
    "    for t in range(1,n_selected):\n",
    "        for i in range(n_obj+n_con):\n",
    "            if i < n_obj:\n",
    "                ecdf_table[t,i,:] = np.sort(df_table['cumulative'][(df_table['objective'] == f'Obj {i}')&(df_table['step'] == f'Step {t}')].values)\n",
    "            else:\n",
    "                ecdf_table[t,i,:] = np.sort(df_table['cumulative'][(df_table['objective'] == f'Con {i - n_obj}')&(df_table['step'] == f'Step {t}')].values)\n",
    "    return ecdf_table\n",
    "\n",
    "def z_transform_from_ecdf(Y, ecdf_table):\n",
    "    N, d = Y.shape  \n",
    "    Y_z = np.empty_like(Y)\n",
    "    for t in range(1,N):\n",
    "        for i in range(d):\n",
    "            ranks = np.searchsorted(ecdf_table[t,i,:], Y[t, i], side='right')\n",
    "            u = ranks / len(ecdf_table[t,i,:])\n",
    "            u = np.clip(u, 1e-12, 1 - 1e-12)\n",
    "            Y_z[t, i] = norm.ppf(u) \n",
    "    return Y_z\n",
    "\n",
    "# def fit_gamma_y(YY):\n",
    "#     N, T, d = YY.shape\n",
    "#     u_YY = np.empty((N, T, d))\n",
    "#     shape = np.empty((T, d))\n",
    "#     location = np.empty((T, d))\n",
    "#     scale = np.empty((T, d))\n",
    "#     for t in range(1, T): # 1-indexed\n",
    "#         for i in range(d):\n",
    "#             a, loc, b = gamma.fit(YY[:, t, i], floc=0.0)\n",
    "#             u = gamma.cdf(YY[:, t, i], a = a, scale = b)\n",
    "#             u = np.clip(u, 1e-12, 1-1e-12)\n",
    "#             u_YY[:, t, i] = u\n",
    "\n",
    "#             shape[t, i] = a\n",
    "#             location[t, i] = loc\n",
    "#             scale[t, i] = b\n",
    "#     return u_YY, shape, location, scale\n",
    "\n",
    "def fit_gamma_y_to_z(YY, XX_z):\n",
    "    N, T, d = YY.shape\n",
    "    YY_z = np.empty((N, T, d), dtype=float)\n",
    "    YY_z[:, 0, :] = XX_z[:, 0, :] \n",
    "    shape = np.full((T-1, d), np.nan)\n",
    "    location = np.full((T-1, d), np.nan)\n",
    "    scale = np.full((T-1, d), np.nan)\n",
    "    for t in range(1, T):\n",
    "        for i in range(d):\n",
    "            y = YY[:, t, i]\n",
    "            y_r = y + 1\n",
    "            \n",
    "            # check\n",
    "            # if not np.all(np.isfinite(y_r)):\n",
    "            #     raise ValueError(\"y_r has non-finite values\")\n",
    "            # if np.any(y_r <= 0):\n",
    "            #     raise ValueError(f\"y_r has non-positive values, min={y_r.min()}\")\n",
    "            # if np.var(y_r) == 0:\n",
    "            #     raise ValueError(\"y_r is constant; gamma MLE is ill-posed\")\n",
    "            if np.ptp(y_r) < 1e-12:\n",
    "                raise RuntimeError(\n",
    "                    f\"Gamma degenerate at t={t}, i={i}\\n\"\n",
    "                    f\"unique(y_r)={np.unique(y_r)}\\n\"\n",
    "                    f\"min={y_r.min()}, max={y_r.max()}\\n\"\n",
    "                    f\"population size={YY.shape[0]}\"\n",
    "                    f\"XX_z= {XX_z[:,t,i]}\"\n",
    "                )\n",
    "\n",
    "            a, loc, b = gamma.fit(y_r, floc=0.0)\n",
    "            u = gamma.cdf(y_r, a=a, loc=loc, scale=b)\n",
    "            u = np.clip(u, 1e-12, 1 - 1e-12)\n",
    "            YY_z[:, t, i] = norm.ppf(u)\n",
    "            shape[t-1, i] = a\n",
    "            location[t-1, i] = loc\n",
    "            scale[t-1, i] = b\n",
    "    return YY_z, shape, location, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85382a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_cumu_objectives(items, items_z, population, n_selected, n_obj, n_con, rng, if_inital): \n",
    "# can modify to use objectives instead of population\n",
    "    XX = np.empty((population.shape[0], n_selected, n_obj+n_con))\n",
    "    XX_z = np.empty((population.shape[0], n_selected, n_obj+n_con))\n",
    "    for k in range(population.shape[0]):\n",
    "        if if_inital:\n",
    "            qx = rng.permutation(population[k, :]) # permutation only for initial population\n",
    "        else:\n",
    "            qx = population[k, :]\n",
    "        XX[k,:,:] = items[qx,:]\n",
    "        XX_z[k,:,:] = items_z[qx,:]\n",
    "    YY = np.cumsum(XX, axis = 1)\n",
    "    YY_z, shape, location, scale = fit_gamma_y_to_z(YY, XX_z)\n",
    "    # df_Y = convert_to_long(YY, n_selected, n_obj, n_con)\n",
    "    # ecdf_table = ecdf(df_Y, n_selected, n_obj, n_con)\n",
    "    # YY_z = np.empty_like(YY)\n",
    "    # for k in range(YY.shape[0]):\n",
    "    #     YY_z[k] = z_transform_from_ecdf(YY[k], ecdf_table)\n",
    "    #     YY_z[k,0,:] = XX_z[k,0,:]\n",
    "    # YY_z = np.cumsum(XX_z, axis = 1)\n",
    "    return XX, XX_z, YY, YY_z, shape, location, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f71c5a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_markov_in_y_by_t(X, Y):\n",
    "    K, N, d = X.shape\n",
    "    A_list = np.zeros((N-1, d, d))\n",
    "    b_list = np.zeros((N-1, d))\n",
    "    Q_list = np.zeros((N-1, d, d))\n",
    "    R2_list = np.zeros(N-1)\n",
    "    reg_list = []\n",
    "\n",
    "    for t in range(1, N):  \n",
    "        S_t = Y[:, t-1, :]  \n",
    "        Z_t = X[:, t,   :] \n",
    "        reg_t = LinearRegression(fit_intercept=True)\n",
    "        reg_t.fit(S_t, Z_t)\n",
    "        A_t = reg_t.coef_      # (d, d)\n",
    "        b_t = reg_t.intercept_ # (d,)\n",
    "        Z_hat_t = reg_t.predict(S_t)\n",
    "        R_t = Z_t - Z_hat_t\n",
    "        Q_t = np.cov(R_t, rowvar=False, bias=False)\n",
    "        r2 = reg_t.score(S_t, Z_t)\n",
    "\n",
    "        A_list[t-1, :, :] = A_t\n",
    "        b_list[t-1, :] = b_t\n",
    "        Q_list[t-1, :, :] = Q_t\n",
    "        R2_list[t-1] = r2\n",
    "        reg_list.append(reg_t)\n",
    "    params = {\"A\": A_list,\"b\": b_list,\"Q\": Q_list,\"regs\": reg_list,\"R2\": R2_list}\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa40195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_conditional(items, items_z, population, n_selected, n_obj, n_con, rng, if_inital):\n",
    "    objectives, objectives_z, cumu_objectives, cumu_objectives_z, shape, location, scale = get_norm_cumu_objectives(items, items_z, population, \n",
    "                                                                                                                        n_selected, n_obj, n_con, \n",
    "                                                                                                                        rng, if_inital)\n",
    "    dist_params = fit_markov_in_y_by_t(objectives_z, cumu_objectives_z)\n",
    "    return objectives_z, dist_params, shape, location, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c571206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_density_given_Y_and_t(X_candidates, y_normal, params_time, t):\n",
    "    A_all = params_time[\"A\"]  \n",
    "    b_all = params_time[\"b\"]  \n",
    "    Q_all = params_time[\"Q\"]  \n",
    "\n",
    "    A_t = A_all[t-1]\n",
    "    b_t = b_all[t-1]\n",
    "    Q_t = Q_all[t-1]\n",
    "    X_candidates = np.asarray(X_candidates)\n",
    "    y_normal = np.asarray(y_normal).reshape(-1)\n",
    "    mean_t = A_t @ y_normal + b_t\n",
    "    Q_t = Q_t + 1e-3 * np.eye(Q_t.shape[0])\n",
    "\n",
    "    densities = mvn.pdf(X_candidates, mean=mean_t, cov=Q_t)\n",
    "    return densities\n",
    "\n",
    "def get_conditional_params(dist_params):\n",
    "    A_all = dist_params[\"A\"]\n",
    "    b_all = dist_params[\"b\"]\n",
    "    Q_all = dist_params[\"Q\"]\n",
    "    \n",
    "    conditional_models = []\n",
    "    for t in range(len(Q_all)):\n",
    "        Q_t = Q_all[t] + 1e-3 * np.eye(Q_all[t].shape[0])\n",
    "        mvn_cond = mvn(mean=np.zeros(Q_t.shape[0]), cov=Q_t)\n",
    "        conditional_models.append({\n",
    "            \"A\": A_all[t],\n",
    "            \"b\": b_all[t],\n",
    "            \"mvn_cond\": mvn_cond\n",
    "        })\n",
    "    return conditional_models\n",
    "\n",
    "def calculate_probs_conditional(model, x_candidates, y_normal):\n",
    "    A_t = model[\"A\"]\n",
    "    b_t = model[\"b\"]\n",
    "    mvn_cond = model[\"mvn_cond\"]\n",
    "    \n",
    "    mean_t = A_t @ y_normal + b_t\n",
    "    centered_candidates = x_candidates - mean_t  # center x candidates to 0\n",
    "    \n",
    "    densities = mvn_cond.pdf(centered_candidates)\n",
    "    probabilities = densities / np.sum(densities)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c92f0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_rate_model(items_z, XX_0):\n",
    "    mean0 = np.mean(XX_0, axis = 0)\n",
    "    Sigma0 = np.cov(XX_0.T) \n",
    "    # add regularization to diagonal for singularity\n",
    "    Sigma0 = Sigma0 + 1e-3 * np.eye(Sigma0.shape[0])\n",
    "    mvn0 = mvn(mean=mean0, cov=Sigma0)\n",
    "    x_candidates = items_z\n",
    "    probabilities = mvn0.pdf(x_candidates)\n",
    "    probabilities = (probabilities+1e-12)/sum(probabilities+1e-12)\n",
    "    return probabilities\n",
    "\n",
    "def get_base_model_params(items_z, XX_0):\n",
    "    mean0 = np.mean(XX_0, axis=0)\n",
    "    Sigma0 = np.cov(XX_0.T)\n",
    "    # may add regularization here\n",
    "    mvn0 = mvn(mean=mean0, cov=Sigma0)\n",
    "    return mvn0\n",
    "\n",
    "def calculate_probs_base(model, x_candidates):\n",
    "    probabilities = model.pdf(x_candidates)\n",
    "    probabilities = (probabilities + 1e-12) / np.sum(probabilities + 1e-12)\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6a7b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_population_conditional(\n",
    "#     samples, samples_z, objectives_z, dist_params,\n",
    "#     pop_size, n_selected, capacity, rng): # no use of rng\n",
    "\n",
    "#     pop_count = 0\n",
    "#     population = np.zeros((pop_size, n_selected), dtype=np.int32)\n",
    "#     n_items = samples.shape[0]\n",
    "\n",
    "#     while pop_count < pop_size:\n",
    "#         # select sequentially\n",
    "#         knapsack = np.zeros(n_selected, dtype=int) # here knapsack is knapsack indices\n",
    "#         for n in range(n_selected):\n",
    "#             if n == 0: # select first item\n",
    "#                 probabilities = base_rate_model(samples_z, objectives_z[:, 0, :])\n",
    "#                 first_choice = rng.choice(n_items, p=probabilities)\n",
    "#                 first_item = samples_z[first_choice,:]\n",
    "#                 x_indices = np.setdiff1d(np.arange(n_items), knapsack)\n",
    "#                 y_prev = first_item \n",
    "#                 knapsack[0] = first_choice\n",
    "#             else:\n",
    "#                 x_indices = np.setdiff1d(np.arange(n_items), knapsack[:n])\n",
    "#                 x_candidates = samples_z[x_indices, :]\n",
    "#                 densities = conditional_density_given_Y_and_t(\n",
    "#                     x_candidates, y_prev, dist_params, n\n",
    "#                 )\n",
    "#                 probabilities = densities/sum(densities)\n",
    "#                 next_choice = rng.choice(len(probabilities), p=probabilities)\n",
    "#                 next_index = x_indices[next_choice]\n",
    "#                 next_item = samples_z[next_index,:]\n",
    "#                 knapsack[n] = next_index\n",
    "#                 y_prev = y_prev + next_item\n",
    "        \n",
    "#         constraint = np.sum(samples[knapsack, -1])\n",
    "#         if constraint <= capacity:\n",
    "#             population[pop_count, :] = knapsack\n",
    "#             pop_count += 1\n",
    "    \n",
    "#     return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d878268d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize y during sampling\n",
    "def sample_population_conditional(\n",
    "    samples, samples_z, objectives_z, dist_params,\n",
    "    shape, location, scale, if_converged,\n",
    "    pop_size, n_selected, n_obj, n_con, capacity, rng):\n",
    "\n",
    "    pop_count = 0\n",
    "    population = np.zeros((pop_size, n_selected), dtype=np.int32)\n",
    "    n_items = samples.shape[0]\n",
    "\n",
    "    while pop_count < pop_size:\n",
    "        knapsack_indices = np.zeros(n_selected, dtype=int)    \n",
    "        knapsack = np.zeros((n_selected,(n_obj+n_con)))\n",
    "\n",
    "        # if if_converged:\n",
    "        #     probabilities = base_rate_model(samples_z, objectives_z[:, 0, :])\n",
    "        # else:\n",
    "        probabilities = np.ones(n_items) / n_items\n",
    "        # probabilities = base_rate_model(samples_z, objectives_z[:, 0, :])\n",
    "        first_choice = rng.choice(n_items, p=probabilities)\n",
    "        knapsack_indices[0] = first_choice\n",
    "        knapsack[0, :] = samples[first_choice, :]\n",
    "        y_prev_z = samples_z[first_choice, :] \n",
    "        y_cum = knapsack[0, :].copy()\n",
    "\n",
    "        for n in range(1, n_selected):\n",
    "            x_indices = np.setdiff1d(np.arange(n_items), knapsack_indices[:n])   \n",
    "            x_candidates = samples_z[x_indices, :] \n",
    "            densities = conditional_density_given_Y_and_t(x_candidates, y_prev_z, dist_params, n)\n",
    "            probabilities = densities/sum(densities)\n",
    "            next_choice = rng.choice(len(probabilities), p=probabilities)\n",
    "            next_index = x_indices[next_choice]\n",
    "            knapsack_indices[n] = next_index\n",
    "            knapsack[n, :] = samples[next_index, :]\n",
    "            \n",
    "            # normalize y\n",
    "            y_cum += knapsack[n, :]\n",
    "            u = gamma.cdf(y_cum, a=shape[n-1, :], loc=location[n-1, :], scale=scale[n-1, :])\n",
    "            u = np.clip(u, 1e-12, 1-1e-12)\n",
    "            y_prev_z = norm.ppf(u)\n",
    "        \n",
    "        constraint = np.sum(samples[knapsack_indices, -1])\n",
    "        if constraint <= capacity:\n",
    "            population[pop_count, :] = knapsack_indices\n",
    "            pop_count += 1\n",
    "    \n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6371d721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_population_conditional_converged(\n",
    "    samples, samples_z, objectives_z, dist_params,\n",
    "    shape, location, scale, if_converged,\n",
    "    pop_size, n_selected, n_obj, n_con, capacity, rng):\n",
    "\n",
    "    pop_count = 0\n",
    "    population = np.zeros((pop_size, n_selected), dtype=np.int32)\n",
    "    n_items = samples.shape[0]\n",
    "\n",
    "    while pop_count < pop_size:\n",
    "        knapsack_indices = np.zeros(n_selected, dtype=int)    \n",
    "        knapsack = np.zeros((n_selected,(n_obj+n_con)))\n",
    "\n",
    "        probabilities = base_rate_model(samples_z, objectives_z[:, 0, :])\n",
    "        first_choice = rng.choice(n_items, p=probabilities)\n",
    "        knapsack_indices[0] = first_choice\n",
    "        knapsack[0, :] = samples[first_choice, :]\n",
    "        y_prev_z = samples_z[first_choice, :] \n",
    "        y_cum = knapsack[0, :].copy()\n",
    "\n",
    "        for n in range(1, n_selected):\n",
    "            x_indices = np.setdiff1d(np.arange(n_items), knapsack_indices[:n])   \n",
    "            x_candidates = samples_z[x_indices, :] \n",
    "            densities = conditional_density_given_Y_and_t(x_candidates, y_prev_z, dist_params, n)\n",
    "            probabilities = densities/sum(densities)\n",
    "            next_choice = rng.choice(len(probabilities), p=probabilities)\n",
    "            next_index = x_indices[next_choice]\n",
    "            knapsack_indices[n] = next_index\n",
    "            knapsack[n, :] = samples[next_index, :]\n",
    "            \n",
    "            # normalize y\n",
    "            y_cum += knapsack[n, :]\n",
    "            u = gamma.cdf(y_cum, a=shape[n-1, :], loc=location[n-1, :], scale=scale[n-1, :])\n",
    "            u = np.clip(u, 1e-12, 1-1e-12)\n",
    "            y_prev_z = norm.ppf(u)\n",
    "        \n",
    "        constraint = np.sum(samples[knapsack_indices, -1])\n",
    "        if constraint <= capacity:\n",
    "            population[pop_count, :] = knapsack_indices\n",
    "            pop_count += 1\n",
    "    \n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d9476d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "# from functools import partial\n",
    "# from scipy.special import gammainc, ndtri\n",
    "\n",
    "# def _sample_chunk(seed, chunk_size, samples, samples_z, base_model_params, cond_models, \n",
    "#                   shape, location, scale,\n",
    "#                   n_selected, n_obj, n_con, capacity):\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     n_items = samples.shape[0]\n",
    "#     population_chunk = np.zeros((chunk_size, n_selected), dtype=np.int32)\n",
    "#     pop_count = 0\n",
    "#     total_attempts = 0\n",
    "    \n",
    "#     while pop_count < chunk_size:\n",
    "#         total_attempts += 1\n",
    "        \n",
    "#         knapsack_indices = np.zeros(n_selected, dtype=int)    \n",
    "#         knapsack = np.zeros((n_selected, (n_obj + n_con)))\n",
    "        \n",
    "#         probabilities = calculate_probs_base(base_model_params, samples_z)\n",
    "#         first_choice = rng.choice(n_items, p=probabilities)\n",
    "#         knapsack_indices[0] = first_choice\n",
    "#         knapsack[0, :] = samples[first_choice, :]\n",
    "#         y_prev_z = samples_z[first_choice, :]     \n",
    "#         for n in range(1, n_selected):\n",
    "#             x_indices = np.setdiff1d(np.arange(n_items), knapsack_indices[:n])   \n",
    "#             x_candidates = samples_z[x_indices, :] \n",
    "#             # step n (1 to N-1) corresponds to cond_models n-1\n",
    "#             probs = calculate_probs_conditional(cond_models[n-1], x_candidates, y_prev_z)\n",
    "#             next_choice_idx = rng.choice(len(probs), p=probs)\n",
    "#             next_index = x_indices[next_choice_idx]\n",
    "#             knapsack_indices[n] = next_index\n",
    "#             knapsack[n, :] = samples[next_index, :]\n",
    "            \n",
    "#             # normalize y\n",
    "#             u = gamma.cdf(knapsack[n, :], a=shape[n, :], loc=location[n, :], scale=scale[n, :])\n",
    "#             # u = gammainc(shape[n, :], knapsack[n, :] / scale[n, :])\n",
    "#             u = np.clip(u, 1e-12, 1-1e-12)\n",
    "#             y_prev_z = norm.ppf(u)\n",
    "     \n",
    "#         constraint = np.sum(samples[knapsack_indices, -1])\n",
    "#         if constraint <= capacity:\n",
    "#             population_chunk[pop_count, :] = knapsack_indices\n",
    "#             pop_count += 1\n",
    "        \n",
    "#         if total_attempts % 5000 == 0:\n",
    "#             print(f\"  Worker pid={multiprocessing.current_process().pid}: \"\n",
    "#                   f\"Sampled {pop_count}/{chunk_size} (Attempts: {total_attempts})\", flush=True)\n",
    "\n",
    "                \n",
    "#     return population_chunk\n",
    "\n",
    "# def sample_population_conditional(\n",
    "#     samples, samples_z, objectives_z, dist_params, \n",
    "#     shape, location, scale,\n",
    "#     pop_size, n_selected, n_obj, n_con, capacity, rng):\n",
    "\n",
    "#     base_model = get_base_model_params(samples_z, objectives_z[:, 0, :])\n",
    "#     cond_models = get_conditional_params(dist_params)\n",
    "    \n",
    "#     n_jobs = multiprocessing.cpu_count()\n",
    "#     chunk_size = pop_size // n_jobs\n",
    "#     remainder = pop_size % n_jobs\n",
    "#     chunks = [chunk_size] * n_jobs\n",
    "#     if remainder > 0:\n",
    "#         chunks[-1] += remainder\n",
    "        \n",
    "#     seeds = rng.integers(0, 1e9, size=n_jobs)\n",
    "#     with multiprocessing.Pool(processes=n_jobs) as pool:\n",
    "#         func = partial(_sample_chunk, \n",
    "#                        samples=samples, \n",
    "#                        samples_z=samples_z, \n",
    "#                        base_model_params=base_model,\n",
    "#                        cond_models=cond_models,\n",
    "#                        shape=shape,\n",
    "#                        location=location,\n",
    "#                        scale=scale,\n",
    "#                        n_selected=n_selected,\n",
    "#                        n_obj=n_obj,\n",
    "#                        n_con=n_con, \n",
    "#                        capacity=capacity)\n",
    "        \n",
    "#         results = pool.starmap(func, zip(seeds, chunks))\n",
    "        \n",
    "#     population = np.vstack(results)\n",
    "#     return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5db27143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnapsackEDACond:\n",
    "    def __init__(self, items, capacity, n_selected, n_obj, n_con, pop_size=1000, \n",
    "                 generations=5, max_no_improve_gen=20, seed=1123):\n",
    "        self.items = items\n",
    "        self.items_z = None\n",
    "        self.capacity = capacity\n",
    "        self.n_selected = n_selected\n",
    "        self.n_obj = n_obj\n",
    "        self.n_con = n_con\n",
    "        self.pop_size = pop_size\n",
    "        self.generations = generations\n",
    "        self.max_no_improve_gen = max_no_improve_gen\n",
    "        self.rng = random.default_rng(seed=seed)\n",
    "        self.if_inital = True\n",
    "        self.ecdf_table = None\n",
    "        self.rank_params = None\n",
    "        self.shape = None\n",
    "        self.location = None\n",
    "        self.scale = None\n",
    "        self.if_converged = False\n",
    "\n",
    "        # self.distribution = None\n",
    "        self.first_item_dist = None\n",
    "        self.distribution_params = None\n",
    "        self.selected_population = None  # (pop_size, n_selected)\n",
    "        self.selected_objectives = None  # (pop_size, n_obj) objective values are summed over solutions\n",
    "        self.objectives_z = None  # (pop_size, n_selected, n_obj)\n",
    "\n",
    "        self.distribution_params_table = []\n",
    "        self.pareto_indices_table = []\n",
    "        self.pareto_front_table = []\n",
    "        self.js_div_list = []\n",
    "        \n",
    "    def _generate_initial_population(self):\n",
    "        n_items = self.items.shape[0]\n",
    "        distribution = np.ones(n_items) / n_items\n",
    "        population = sample_population(\n",
    "            self.items, distribution, self.pop_size, self.n_selected, \n",
    "            self.capacity, self.rng\n",
    "        )\n",
    "        objectives = get_objectives(self.items, population, self.n_obj)\n",
    "        \n",
    "        ranks, fronts = non_dominated_sort(objectives)\n",
    "        distances_all_solutions = np.zeros(population.shape[0], dtype=float)\n",
    "        for f in fronts:\n",
    "            distances = assign_crowding_distance(objectives[f, :])\n",
    "            distances_all_solutions[f] = distances\n",
    "\n",
    "        select_indices = np.array([], dtype=int)\n",
    "        while len(select_indices) < self.pop_size:\n",
    "            indice = binary_tournament_selection(\n",
    "                population, ranks, distances_all_solutions, self.rng\n",
    "            )\n",
    "            select_indices = np.concatenate([select_indices, np.array([indice])])\n",
    "        \n",
    "        selected_population = population[select_indices]\n",
    "        selected_objectives = objectives[select_indices]\n",
    "\n",
    "        _, _, self.items_z = transform_items_to_z(self.items)\n",
    "        self.objectives_z, self.distribution_params, self.shape, self.location, self.scale = fit_conditional(self.items, self.items_z, selected_population, \n",
    "                                                                                                                self.n_selected, self.n_obj, self.n_con,\n",
    "                                                                                                                self.rng, self.if_inital) # may need a different rng\n",
    "        self.first_item_dist = base_rate_model(self.items_z, self.objectives_z[:, 0, :])\n",
    "        self.selected_population = selected_population\n",
    "        self.selected_objectives = selected_objectives\n",
    "        \n",
    "        return #selected_population, selected_objectives, items_z, objectives_z, distribution_params\n",
    "    \n",
    "    def _update_distribution(self):\n",
    "        # sample using uniform\n",
    "        population = sample_population_conditional(\n",
    "            self.items, self.items_z, self.objectives_z, self.distribution_params,\n",
    "            self.shape, self.location, self.scale, self.if_converged,\n",
    "            self.pop_size, self.n_selected, self.n_obj, self.n_con, self.capacity, self.rng\n",
    "        )\n",
    "        objectives = get_objectives(self.items, population, self.n_obj)\n",
    "        \n",
    "        # find current pareto front\n",
    "        _, fronts_current = non_dominated_sort(objectives)\n",
    "        pareto_indices = population[fronts_current[0]]\n",
    "        \n",
    "        objectives = np.vstack((self.selected_objectives, objectives))\n",
    "        population = np.vstack((self.selected_population, population))\n",
    "        \n",
    "        # _, _, ranks, fronts = non_dominated_sort(objectives)\n",
    "        ranks, fronts = non_dominated_sort(objectives)\n",
    "        select_indices = np.array([], dtype=np.int32)\n",
    "        for f in fronts:\n",
    "            if len(select_indices) + len(f) <= self.pop_size:\n",
    "                select_indices = np.concatenate([select_indices, f])\n",
    "            else:\n",
    "                remaining_size = self.pop_size - len(select_indices)\n",
    "                f_distance = assign_crowding_distance(objectives[f, :])\n",
    "                sort_indices = np.argsort(f_distance)[::-1]\n",
    "                remaining = f[sort_indices[:remaining_size]]\n",
    "                select_indices = np.concatenate([select_indices, remaining])\n",
    "                break\n",
    "        \n",
    "        selected_population = population[select_indices]\n",
    "        selected_objectives = objectives[select_indices]\n",
    "        if self.if_converged:\n",
    "            training_population = population[fronts[0]]\n",
    "        else:\n",
    "            n_training = int(self.pop_size*0.15)\n",
    "            training_population = selected_population[:n_training]\n",
    "        \n",
    "        # update distribution\n",
    "        self.objectives_z, self.distribution_params, self.shape, self.location, self.scale= fit_conditional(self.items, self.items_z, training_population, \n",
    "                                                                                                                self.n_selected, self.n_obj, self.n_con,\n",
    "                                                                                                                self.rng, self.if_inital)\n",
    "        \n",
    "        self.selected_population = selected_population\n",
    "        self.selected_objectives = selected_objectives\n",
    "\n",
    "        # check distribution convergence\n",
    "        updated_first_item_dist = base_rate_model(self.items_z, self.objectives_z[:, 0, :])\n",
    "        self.first_item_dist[self.first_item_dist < 1E-08] = 1E-08\n",
    "        updated_first_item_dist[updated_first_item_dist < 1E-08] = 1E-08\n",
    "        js_div = jensenshannon(self.first_item_dist, updated_first_item_dist)**2\n",
    "        self.first_item_dist = updated_first_item_dist\n",
    "        # if js_div < 1e-4:\n",
    "        #     self.if_converged = True\n",
    "        #     print(\"Converged\")\n",
    "        \n",
    "        return pareto_indices, js_div\n",
    "\n",
    "    # def _converged_pf(self):\n",
    "    #     # sample using base rate model\n",
    "    #     population = sample_population_conditional_converged(\n",
    "    #         self.items, self.items_z, self.objectives_z, self.distribution_params,\n",
    "    #         self.shape, self.location, self.scale, self.if_converged,\n",
    "    #         self.pop_size, self.n_selected, self.n_obj, self.n_con, self.capacity, self.rng)\n",
    "    #     objectives = get_objectives(self.items, population, self.n_obj)\n",
    "\n",
    "    #     # find current pareto front\n",
    "    #     pareto_indices = population[non_dominated(objectives).astype(bool)]\n",
    "        \n",
    "    #     objectives = np.vstack((self.selected_objectives, objectives))\n",
    "    #     population = np.vstack((self.selected_population, population))\n",
    "       \n",
    "    #     nd_idx = non_dominated(objectives).astype(bool)\n",
    "    #     selected_population = population[nd_idx]\n",
    "    #     selected_objectives = objectives[nd_idx]\n",
    "    #     self.objectives_z, self.distribution_params, self.shape, self.location, self.scale = fit_conditional(self.items, self.items_z, selected_population, \n",
    "    #                                                                                                             self.n_selected, self.n_obj, self.n_con,\n",
    "    #                                                                                                             self.rng, self.if_inital)\n",
    "    #     self.selected_population = selected_population\n",
    "    #     self.selected_objectives = selected_objectives                                                                             \n",
    "        \n",
    "    #     return pareto_indices\n",
    " \n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        self._generate_initial_population()\n",
    "        self.if_inital = False\n",
    "        \n",
    "        # Run generations (fixed number of generations)\n",
    "        for g in range(self.generations):\n",
    "            print(f\"Generation {g+1}/{self.generations}\")\n",
    "            pareto_indices, js_div = self._update_distribution()\n",
    "            print(f\"number of front 0: {pareto_indices.shape[0]}\")\n",
    "            \n",
    "            pareto_front = np.zeros((pareto_indices.shape[0], self.items.shape[1]))\n",
    "            for k in range(pareto_indices.shape[0]):\n",
    "                pareto_front[k, :] = np.sum(self.items[pareto_indices[k, :], :], axis=0)\n",
    "                \n",
    "            self.distribution_params_table.append(self.distribution_params.copy())\n",
    "            self.pareto_indices_table.append(pareto_indices.copy())\n",
    "            self.pareto_front_table.append(pareto_front.copy())\n",
    "            self.js_div_list.append(js_div)\n",
    "\n",
    "        return {\n",
    "            'distribution_params_table': self.distribution_params_table,\n",
    "            'pareto_indices_table': self.pareto_indices_table,\n",
    "            'pareto_front_table': self.pareto_front_table,\n",
    "            'js_div_list': self.js_div_list,\n",
    "            'objectives_z': self.objectives_z,\n",
    "            'items_z': self.items_z,\n",
    "            'shape': self.shape,\n",
    "            'location': self.location,\n",
    "            'scale': self.scale\n",
    "        }\n",
    "        \n",
    "        # # Run generations (until convergence)\n",
    "        # no_improve_gen = 0\n",
    "        # prev_js_div = None\n",
    "        # generation = 0\n",
    "        # while no_improve_gen < self.max_no_improve_gen:\n",
    "        #     generation += 1\n",
    "        #     print(f\"Generation {generation} (no improve count: {no_improve_gen})\")\n",
    "        #     pareto_indices, js_div = self._update_distribution()\n",
    "        #     print(f\"number of front 0: {pareto_indices.shape[0]}\")\n",
    "\n",
    "        #     pareto_front = np.zeros((pareto_indices.shape[0], self.items.shape[1]))\n",
    "        #     for k in range(pareto_indices.shape[0]):\n",
    "        #         pareto_front[k, :] = np.sum(self.items[pareto_indices[k, :], :], axis=0)\n",
    "                \n",
    "        #     self.distribution_params_table.append(self.distribution_params.copy())\n",
    "        #     self.pareto_indices_table.append(pareto_indices.copy())\n",
    "        #     self.pareto_front_table.append(pareto_front.copy())\n",
    "        #     self.js_div_list.append(js_div)\n",
    "                \n",
    "        #     if prev_js_div is not None:\n",
    "        #         diff = prev_js_div - js_div\n",
    "        #         if np.abs(diff) > 0.0001: # lowered criteria\n",
    "        #             no_improve_gen = 0\n",
    "        #         else:\n",
    "        #             no_improve_gen += 1\n",
    "        #     else:\n",
    "        #         no_improve_gen = 0\n",
    "        #     prev_js_div = js_div\n",
    "\n",
    "        # return {\n",
    "        #     'distribution_params_table': self.distribution_params_table,\n",
    "        #     'pareto_indices_table': self.pareto_indices_table,\n",
    "        #     'pareto_front_table': self.pareto_front_table,\n",
    "        #     'js_div_list': self.js_div_list,\n",
    "        #     'objectives_z': self.objectives_z,\n",
    "        #     'items_z': self.items_z,\n",
    "        #     'shape': self.shape,\n",
    "        #     'location': self.location,\n",
    "        #     'scale': self.scale\n",
    "        # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4416ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# items_seed = 1211\n",
    "# # Generate data\n",
    "# items, rpos = generate_example_data(r, shape, scale, n_items=n_items, seed=items_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "244ce5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a test case\n",
    "kn = loadmat('/home/tailai/data/knapsack/runB/kn_2_2_allneg_60_6_3.mat')\n",
    "items = kn['items'][1]\n",
    "shape = kn['shape']\n",
    "scale = kn['scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a90d6619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "eda_seed = 1223\n",
    "n_items = 60\n",
    "n_selected = 6\n",
    "n_obj = 3\n",
    "n_con = 1\n",
    "capacity = int(shape[-1]*scale[-1]*n_selected)\n",
    "pop_size = 1000\n",
    "generations = 100 # do not matter if check convergence\n",
    "max_no_improve_gen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c1d146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run EDA\n",
    "eda = KnapsackEDACond(\n",
    "    items=items,\n",
    "    capacity=capacity,\n",
    "    n_selected=n_selected,\n",
    "    n_obj=n_obj,\n",
    "    n_con=n_con,\n",
    "    pop_size=pop_size,\n",
    "    generations=generations,\n",
    "    max_no_improve_gen=max_no_improve_gen,\n",
    "    seed=eda_seed\n",
    ")\n",
    "\n",
    "#organize results    \n",
    "results = eda.run()\n",
    "with open('results_cond_uni_base_2_2.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b579d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('results_cond_uni_base_3_3.pkl', 'rb') as f:\n",
    "#     results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b412443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(results['js_div_list'])\n",
    "plt.ylim(0, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b185cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "A_all = np.stack([gen['A'] for gen in results['distribution_params_table']])\n",
    "for m in range(5):\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 10), sharex=True)\n",
    "    fig.suptitle(f\"Model {m+1}: Coefficient trajectories\", fontsize=14)\n",
    "\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            ax = axes[i, j]\n",
    "            ax.plot(A_all[:, m, i, j])\n",
    "            ax.set_title(f\"A[{i},{j}]\", fontsize=9)\n",
    "            ax.set_ylim(-1, 1)\n",
    "\n",
    "            if i == 3:\n",
    "                ax.set_xlabel(\"Generation\")\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(\"Value\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_params = results['distribution_params_table'][-1]\n",
    "pareto_solutions = results['pareto_indices_table'][-1]\n",
    "objectives_z = results['objectives_z']\n",
    "items_z = results['items_z']\n",
    "shape_eda = results['shape']\n",
    "location_eda = results['location']\n",
    "scale_eda = results['scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadecd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converged_pf_from_dist(\n",
    "    dist_params, shape, location, scale,\n",
    "    items, items_z, objectives_z, pareto_solutions, \n",
    "    capacity, n_selected, n_obj, n_con, f_seed=1234, \n",
    "    sample_size=1000, max_iters=100, max_no_change=2):\n",
    "\n",
    "    if_converged = None\n",
    "    rng = np.random.default_rng(f_seed)       \n",
    "    pareto_solutions = np.unique(np.sort(pareto_solutions, axis=1), axis=0)\n",
    "    pareto_objectives = get_objectives(items, pareto_solutions, n_obj)\n",
    "\n",
    "    no_change = 0\n",
    "    counter = 0\n",
    "    while no_change < max_no_change and counter < max_iters:\n",
    "        new_sample = sample_population_conditional_converged(\n",
    "            items, items_z, objectives_z, dist_params,\n",
    "            shape, location, scale, if_converged,\n",
    "            pop_size, n_selected, n_obj, n_con, capacity, rng)\n",
    "        \n",
    "        all_solutions = np.unique(np.sort(np.vstack((pareto_solutions, new_sample)), axis=1), axis=0)\n",
    "        all_objectives = get_objectives(items, all_solutions, n_obj)\n",
    "        nd_idx = non_dominated(all_objectives)\n",
    "        nd_idx = nd_idx.astype(bool)\n",
    "        new_pareto_solutions = all_solutions[nd_idx]\n",
    "        new_pareto_objectives = all_objectives[nd_idx]\n",
    "        \n",
    "        if np.array_equal(np.unique(new_pareto_objectives, axis=0), np.unique(pareto_objectives, axis=0)):\n",
    "            no_change += 1\n",
    "        else:\n",
    "            no_change = 0\n",
    "\n",
    "        pareto_solutions, pareto_objectives = new_pareto_solutions, new_pareto_objectives\n",
    "        counter += 1\n",
    "        print(f\"iter {counter}: {len(pareto_solutions)}\")\n",
    "        \n",
    "        # update the conditional prob model\n",
    "        if_inital = False\n",
    "        objectives_z, dist_params, shape, location, scale= fit_conditional(items, items_z, pareto_solutions, \n",
    "                                                                                        n_selected, n_obj, n_con,\n",
    "                                                                                        rng, if_inital)\n",
    "    \n",
    "    return pareto_solutions, pareto_objectives, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e27446",
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_solutions, pareto_objectives, counter = converged_pf_from_dist(\n",
    "    dist_params, shape_eda, location_eda, scale_eda,\n",
    "    items, items_z, objectives_z, pareto_solutions, \n",
    "    capacity, n_selected, n_obj, n_con,\n",
    "    max_no_change=5)\n",
    "converged_results = {'pareto_solutions': pareto_solutions, 'pareto_objectives': pareto_objectives}\n",
    "with open('converged_pf_cond_uni_base_2_2_update.pkl', 'wb') as f:\n",
    "    pickle.dump(converged_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e661c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('converged_pf_cond_train_long.pkl', 'rb') as f:\n",
    "#     converged_results_long = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e57613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare last generation's pareto front and converged pareto front\n",
    "pf = results['pareto_front_table'][-1]\n",
    "converged_pf = converged_results['pareto_objectives']\n",
    "pf_unique = np.unique(pf[:, :3], axis=0)\n",
    "pf_unique = pf_unique.astype(int)\n",
    "converged_pf_unique = np.unique(converged_pf, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f58b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for row_p in pf_unique:\n",
    "    for row_c in converged_pf_unique:\n",
    "        if np.array_equal(row_p, row_c):\n",
    "            count += 1\n",
    "            break\n",
    "count/converged_pf_unique.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55f8ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib widget\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot(pf_unique[:,0], pf_unique[:,1], pf_unique[:,2], 'bo', alpha=0.3, markersize=5)\n",
    "ax.plot(converged_pf_unique[:,0], converged_pf_unique[:,1], converged_pf_unique[:,2], 'ro', alpha=0.3, markersize=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a8d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_front_final = kn['pareto_front_final'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d24288",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib widget\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.plot(converged_pf[:,0], converged_pf[:,1], converged_pf[:,2], 'go', alpha=0.5, markersize=5)\n",
    "ax.plot(pareto_front_final[:,0], pareto_front_final[:,1], pareto_front_final[:,2], 'bo', alpha=0.5, markersize=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a86ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj1 = converged_pf\n",
    "obj2 = pareto_front_final[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf889b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.indicators.hv import HV\n",
    "A = obj1.astype(np.float64)\n",
    "B = obj2.astype(np.float64)\n",
    "A_min = -A\n",
    "B_min = -B\n",
    "\n",
    "worst_min = np.max(np.vstack([A_min, B_min]), axis=0)\n",
    "ref = worst_min * 1.05\n",
    "\n",
    "hv = HV(ref_point=ref)\n",
    "\n",
    "A_hv = hv(A_min)\n",
    "B_hv = hv(B_min)\n",
    "\n",
    "print(A_hv)\n",
    "print(B_hv)\n",
    "print((A_hv-B_hv)/B_hv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numba_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
